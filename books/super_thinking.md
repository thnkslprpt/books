# Super Thinking
Author: Gabriel Weinberg  
[Amazon](https://amzn.to/3iNNtOu)  
[Goodreads](https://www.goodreads.com/book/show/41181911-super-thinking)  

>Think back to when you first learned multiplication. As you may recall, multiplication is just repeated addition. In fact, all mathematical operations based on arithmetic can be reduced to just addition: subtraction is just adding a negative number, division is just repeated subtraction, and so on. However, using addition for complex operations can be really slow, which is why you use multiplication in the first place. For example, suppose you have a calculator or spreadsheet in front of you. When you have 158 groups of 7 and you want to know the total, you could use your tool to add 7 to itself 158 times (slow), or you could just multiply 7 × 158 (quick). Using addition is painfully time-consuming when you are aware of the higher-level concept of multiplication, which helps you work quickly and efficiently. When you don’t use mental models, strategic thinking is like using addition when multiplication is available to you. You start from scratch every time without using these essential building blocks that can help you reason about problems at higher levels. And that’s exactly why knowing the right mental models unlocks super thinking, just as subtraction, multiplication, and division unlock your ability to do more complex math problems. Once you have internalized a mental model like multiplication, it’s hard to imagine a world without it. But very few mental models are innate. There was a time when even addition wasn’t known to most people, and you can still find whole societies that live without it. The Pirahã of the Amazon rain forest in Brazil, for example, have no concept of specific numbers, only concepts for “a smaller amount” and “a larger amount.” As a result, they cannot easily count beyond three, let alone do addition, as Brian Butterworth recounted in an October 20, 2004, article for The Guardian, “What Happens When You Can’t Count Past Four?”: Not having much of number vocabulary, and no numeral symbols, such as one, two, three, their arithmetical skills could not be tested in the way we would test even five-year-olds in Britain. Instead, [linguist Peter] Gordon used a matching task. He would lay out up to eight objects in front of him on a table, and the Pirahã participant’s task was to place the same number of objects in order on the table. Even when the objects were placed in a line, accuracy dropped off dramatically after three objects.

>Carl Jacobi was a nineteenth-century German mathematician who often used to say, “Invert, always invert” (actually he said, “Man muss immer umkehren,” because English wasn’t his first language). He meant that thinking about a problem from an inverse perspective can unlock new solutions and strategies. For example, most people approach investing their money from the perspective of making more money; the inverse approach would be investing money from the perspective of not losing money.

>The concept of inverse thinking can help you with the challenge of making good decisions. The inverse of being right more is being wrong less.

>Let us offer an example from the world of sports. In tennis, an unforced error occurs when a player makes a mistake not because the other player hit an awesome shot, but rather because of their own poor judgment or execution. For example, hitting an easy ball into the net is one kind of unforced error. To be wrong less in tennis, you need to make fewer unforced errors on the court. And to be consistently wrong less in decision making, you consistently need to make fewer unforced errors in your own life. See how this works? Unforced error is a concept from tennis, but it can be applied as a metaphor in any situation where an avoidable mistake is made. There are unforced errors in baking (using a tablespoon instead of a teaspoon) or dating (making a bad first impression) or decision making (not considering all your options). Start looking for unforced errors around you and you will see them everywhere.

>Ockham’s razor helps here. It advises that the simplest explanation is most likely to be true. When you encounter competing explanations that plausibly explain a set of data equally well, you probably want to choose the simplest one to investigate first. This model is a razor because it “shaves off” unnecessary assumptions. It’s named after fourteenth-century English philosopher William of Ockham, though the underlying concept has much older roots. The Greco-Roman astronomer Ptolemy (circa A.D. 90–168) stated, “We consider it a good principle to explain the phenomena by the simplest hypotheses possible.” More recently, the composer Roger Sessions, paraphrasing Albert Einstein, put it like this: “Everything should be made as simple as it can be, but not simpler!” In medicine, it’s known by this saying: “When you hear hoofbeats, think of horses, not zebras.”

>The third story, most respectful interpretation, and Hanlon’s razor are all attempts to overcome what psychologists call the fundamental attribution error, where you frequently make errors by attributing others’ behaviors to their internal, or fundamental, motivations rather than external factors.

>The work of Ignaz Semmelweis, a nineteenth-century Hungarian doctor, met a similar fate. He worked at a teaching hospital where doctors routinely handled cadavers and also delivered babies, without appropriately washing their hands in between. The death rate of mothers who gave birth in this part of the hospital was about 10 percent! In another part of the same hospital, where babies were mostly delivered by midwives who did not routinely handle cadavers, the comparable death rate was 4 percent. Semmelweis obsessed about this difference, painstakingly eliminating all variables until he was left with just one: doctors versus midwives. After studying doctor behavior, he concluded that it must be due to their handling of the cadavers and instituted a practice of washing hands with a solution of chlorinated lime. The death rate immediately dropped to match that in the other part of the hospital. Despite the clear drop in the death rate, his theories were completely rejected by the medical community at large. In part, doctors were offended by the idea that they were killing their patients. Others were so hung up on the perceived deficiencies of Semmelweis’s theoretical explanation that they ignored the empirical evidence that the handwashing was improving mortality. After struggling to get his ideas adopted, Semmelweis went crazy, was admitted to an asylum, and died at the age of forty-seven. It took another twenty years after his death for his ideas about antiseptics to start to take hold, following Louis Pasteur’s unquestionable confirmation of germ theory. Like Wegener, Semmelweis didn’t fully understand the scientific mechanism that underpinned his theory and crafted an initial explanation that turned out to be somewhat incorrect. However, they both noticed obvious and important empirical truths that should have been investigated by other scientists but were reflexively rejected by these scientists because the suggested explanations were not in line with the conventional thinking of the time. Today, this is known as a Semmelweis reflex.

>The pernicious effects of confirmation bias and related models can be explained by cognitive dissonance, the stress felt by holding two contradictory, dissonant, beliefs at once.

>F. Scott Fitzgerald once described something similar to thinking gray when he observed that the test of a first-rate mind is the ability to hold two opposing thoughts at the same time while still retaining the ability to function.

>Goodhart’s law summarizes the issue: When a measure becomes a target, it ceases to be a good measure. This more common phrasing is from Cambridge anthropologist Marilyn Strathern in her 1997 paper “‘Improving Ratings’: Audit in the British University System.” However, the “law” is named after English economist Charles Goodhart, whose original formulation in a conference paper presented at the Reserve Bank of Australia in 1975 stated: “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.”

>Both describe the same basic phenomenon: When you try to incentivize behavior by setting a measurable target, people focus primarily on achieving that measure, often in ways you didn’t intend. Most importantly, their focus on the measure may not correlate to the behavior you hoped to promote.

>In A Short History of Nearly Everything, Bill Bryson describes a situation in which paleontologist Gustav Heinrich Ralph von Koenigswald accidentally created perverse incentives on an expedition: Koenigswald’s discoveries might have been more impressive still but for a tactical error that was realized too late. He had offered locals ten cents for every piece of hominid bone they could come up with, then discovered to his horror that they had been enthusiastically smashing large pieces into small ones to maximize their income. It’s like a wish-granting genie who finds loopholes in your wishes, meeting the letter of the wish but not its spirit, and rendering you worse off than when you started. In fact, there is a mental model for this more specific situation, called the cobra effect, describing when an attempted solution actually makes the problem worse. This model gets its name from a situation involving actual cobras. When the British were governing India, they were concerned about the number of these deadly snakes, and so they started offering a monetary reward for every snake brought to them. Initially the policy worked well, and the cobra population decreased. But soon, local entrepreneurs started breeding cobras just to collect the bounties. After the government found out and ended the policy, all the cobras that were being used for breeding were released, increasing the cobra population even further.

>A related model to watch out for is the hydra effect, named after the Lernaean Hydra, a beast from Greek mythology that grows two heads for each one that is cut off. When you arrest one drug dealer, they are quickly replaced by another who steps in to meet the demand. When you shut down an internet site where people share illegal movies or music, more pop up in its place. Regime change in a country can result in an even worse regime.

>With all these traps—Goodhart’s law, along with the cobra, hydra, and Streisand effects—if you are going to think about changing a system or situation, you must account for and quickly react to the clever ways people may respond. There will often be individuals who try to game the system or otherwise subvert what you’re trying to do for their personal gain or amusement.

>Sometimes collateral damage can impact the entity that inflicted the damage in the first place, which is called blowback. Blowback sometimes can occur well after the initial action. The U.S. supported Afghan insurgents in the 1980s against the USSR. Years later these same groups joined al-Qaeda to fight against the U.S., using some of the very same weapons the U.S. had provided decades earlier.

>There is a natural conflict between the desire to make decisions quickly and the feeling that you need to accumulate more information to be sure you are making the right choice. You can deal with this conflict by categorizing decisions as either reversible decisions or irreversible decisions. Irreversible decisions are hard if not impossible to unwind. And they tend to be really important. Think of selling your business or having a kid. This model holds that these decisions require a different decision-making process than their reversible counterparts, which should be treated much more fluidly.

>Loss aversion can be better understood using the frame of reference model (see Chapter 1). When you already have a win on your hands, you tend to want to lock in your gains. From this frame of reference, you tend to act more conservatively and are more likely to pass up a chance at a bigger gain if it means risking your current winnings. Conversely, when you have a loss on your hands, you’d rather take a chance at breaking even than accept the sure loss. From this frame of reference, you tend to act more aggressively, not wanting to end with the loss. From an objective frame of reference, however, you should approach both situations from the same standpoint of opportunity cost. By holding on to a loss too long, you are misallocating time or money that could be better used on another opportunity. Similarly, by walking away after a sure but small gain, you may be missing out on a potentially better opportunity.

>Momentum is a model that can help you understand how things change. Momentum and inertia are related concepts. In physics, momentum is the product (multiplication) of mass and velocity, whereas inertia is just a function of mass. That means a heavy object at rest has a lot of inertia since it is hard to move, but it has no momentum since its velocity is zero. However, a heavy object gets momentum quickly once it starts moving. The faster an object goes, the more momentum it has. However, its inertia remains the same (since its mass remains the same), and it is still similarly difficult to change its velocity.

>When discovering the atomic critical mass, Otto Frisch narrowly avoided a catastrophic chain reaction, known more generally as a cascading failure, where a failure in one piece of a system can trigger a chain reaction of failure that cascades through the entire system.

>A few other examples can further illustrate how subtle survivorship bias can be. In World War II, naval researchers conducted a study of damaged aircraft that returned from missions, so that they could make suggestions as to how to bolster aircraft defenses for future missions. Looking at where these planes had been hit, they concluded that areas where they had taken the most damage should receive extra armor. However, statistician Abraham Wald noted that the study sampled only planes that had survived missions, and not the many planes that had been shot down. He therefore theorized the opposite conclusion, which turned out to be correct: that the areas with holes represented areas where aircraft could be shot and still return safely, whereas the areas without holes probably contained areas that, if hit, would cause the planes to go down.

>In normal distributions like these (and as we saw with the body temperatures), approximately 68 percent of all values should fall within one standard deviation of the mean, about 95 percent within two, and nearly all (99.7 percent) within three. In this manner, a normal distribution can be uniquely described by just its mean and standard deviation. Because so many phenomena can be described by the normal distribution, knowing these facts is particularly useful.

>As another example, consider a mammogram, a medical test used in the diagnosis of breast cancer. You might think a test like this has two possible results: positive or negative. But really a mammogram has four possible outcomes, depicted in the following table. The two possible outcomes you immediately think of are when the test is right, the true positive and the true negative; the other two outcomes occur when the test is wrong, the false positive and the false negative. Possible Test Outcomes Results of mammogram Evidence of cancer No evidence of cancer Patient has breast cancer True positive False negative Patient does not have breast cancer False positive True negative

>You’ve probably heard the phrase If all you have is a hammer, everything looks like a nail. This phrase is called Maslow’s hammer and is derived from this longer passage by psychologist Abraham Maslow in his 1966 book The Psychology of Science: I remember seeing an elaborate and complicated automatic washing machine for automobiles that did a beautiful job of washing them. But it could do only that, and everything else that got into its clutches was treated as if it were an automobile to be washed. I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail.

>In game theory, diagrams can help you study your options. One example is called a payoff matrix, showing the payoffs for possible player choices in matrix form (see 2 × 2 matrix in Chapter 4). From the prisoner’s perspective, the payoff matrix looks like this: Prisoner’s Dilemma Payoff Matrix: Sentences Received B remains silent B betrays A A remains silent 1 year, 1 year 10 years, 0 years A betrays B 0 years, 10 years 5 years, 5 years Here’s where it gets interesting. The simplest formulation of this game assumes that the consequences for the players are only the prison sentences listed, i.e., there is no consideration of real-time negotiation or future retribution. If, as a player, you are acting independently and rationally, the dominant strategy given this formulation and payoff matrix is always to betray your partner: No matter what they do, you’re better off betraying, and that’s the only way to get off free. If your co-conspirator remains silent, you go from one to zero years by betraying them, and if they betray you too, you go from ten to five years. The rub is that if your co-conspirator follows the same strategy, you both go away for much longer than if you both just remained silent (five years versus one year). Hence the dilemma: do you risk their betrayal, or can you trust their solidarity and emerge with a small sentence? The dual betrayal with its dual five-year sentences is known as the Nash equilibrium of this game, named after mathematician John Nash, one of the pioneers of game theory and the subject of the biopic A Beautiful Mind. The Nash equilibrium is a set of player choices for which a change of strategy by any one player would worsen their outcome. In this case, the Nash equilibrium is the strategy of dual betrayals, because if either player instead chose to remain silent, that player would get a longer sentence. To both get a shorter sentence, they’d have to act cooperatively, coordinating their strategies. That coordinated strategy is unstable (i.e., not an equilibrium) because either player could then betray the other to better their outcome. In any game you play, you want to know whether there is a Nash equilibrium, as that is the most likely outcome unless something is done to change the parameters of the game. For example, the Nash equilibrium for an arms race is choosing a high arms strategy where both parties continue to arm themselves. Here’s an example of a payoff matrix for this scenario: Arms Race Payoff Matrix: Economic Outcomes B disarms B arms A disarms win, win lose big, win big A arms win big, lose big lose, lose As you can see, the arms race directly parallels the prisoner’s dilemma. Both A and B arming (the lose-lose situation) is the Nash equilibrium, because if either party switched to disarming, they’d be worse off, enabling an even poorer outcome, such as an invasion they couldn’t defend against (denoted as “lose big”). The best outcome again results from being cooperative, with both parties agreeing to disarm (the win-win situation), thus opening up the opportunity to spend those resources more productively. That’s the arms race equivalent of remaining silent, but it is also an unstable situation, since either side could then better their situation by arming again (and potentially invading the other side, leading to a “win big” outcome).