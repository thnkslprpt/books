# The Big Questions The Universe
Author: Stuart Clark  
[Amazon](https://amzn.to/3C9KJnq)  
[Goodreads](https://www.goodreads.com/book/show/9286353-the-big-questions-the-universe)  

>When you stand at a truly dark site –in a desert or some other wilderness where the only light to be seen is coming from the stars above –the stars fill the sky in such profusion that even the most familiar of the constellations is difficult to pick out. Although there may seem to be countless stars, in fact the human eye can resolve about 3000 under pristine conditions. This is but the tiniest fraction of the total number of stars in the Universe. It has long been a cliché to say that the number of stars in the Universe is the same as the number of grains of sand on all the beaches in the world, but whilst there is indeed a staggering quantity of sand grains on Earth, this total is not nearly large enough. According to the latest estimates there are some 70 sextillion stars in the entire Universe; that is 70 thousand million million million, or a seven followed by 22 zeros. To pursue the comparison, this roughly equates to the number of grains of sand to be found on the beaches of 10,000 Earth-like planets.

>What is the Universe?

>The urge to understand the Universe developed early in human history. Babylonian stone tablets dating back to 3000–3500BC have been found that record the variable length of day throughout the year; the Chinese have written records of eclipses since about 2000BC. Around the world are the remains of prehistoric structures that display striking astronomical alignments. The oldest of these is the 5200-year-old tomb at Newgrange, Ireland. At dawn on the winter solstice, the shortest day of the year, the rising Sun shines its beam through a passageway onto the floor of the inner chamber. On Easter Island in the Pacific, seven of the hundreds of enigmatic statues face in the direction where the Sun sets on the equinox, when night and day have equal length. It has been suggested that the great Cambodian Temple Angkor Wat is aligned so that the Sun rises over the eastern gate on midsummer’s day. The pyramids of Egypt are also thought to show alignment with the stars. Whilst none of these constitute an observatory in a scientific sense, they clearly demonstrate that the builders had an understanding of the motion of the Sun and stars. The earliest astronomical observations were almost certainly used by ancient humans to set the calendar. The phases of the Moon defined the passing of a month, and the passage of the Sun through the sky defined both the length of a day and of a year. As the year progresses, the Sun rises and sets from different points on the horizon. Stonehenge, the well-preserved stone circle on Salisbury Plain in England, has a well-known solar alignment. On midsummer’s day, the Sun rises over an offset monolith known as the Heel Stone. Initially Stonehenge was thought to be a temple to the Sun god, but some researchers have found other alignments between the stones and the Moon and suggested that it could have been a prehistoric observatory, perhaps used primarily for the prediction of eclipses.

>Early civilizations told stories inspired by the patterns of stars in the night sky. They imagined the lines joining stars to form pictures of familiar or mythical characters. In Mesopotamia (modern-day Iraq) archaeologists have unearthed stone tablets and clay ledgers dating back to 1300BC, which detail many such ‘constellations’ including the 12 signs of the zodiac. These zodiacal constellations were given special significance because they inhabited regions of the sky through which the Sun passed, and they were subsequently adopted by the Greeks –the Assyrian Hired Man and the Swallow became Aries and Pisces, for example, and the Goatfish and the Great Twins became Capricorn and Gemini. In Ancient Greece wandering minstrels would drift from village to village, recounting the star myths in exchange for food and lodgings. At the same time, philosophers would come up with their own fanciful tales to explain the nature of the Universe. One of the earliest was the philosopher Thales, in the sixth century BC. He put forward the idea that space was filled with water, in which the Earth floats, that earthquakes were caused by waves in this water, and the stars moved because they were caught in gentler currents.

>‘Astronomy compels the soul to look upwards and leads us from this world to another.' PLATO 4TH CENTURY BC GREEK PHILOSOPHER

>The Greek astronomer Claudius Ptolemy, who lived in the first century AD, compiled a list of 48 constellations, but since not all the sky could be seen from Greece, the regions around the South Pole remained uncharted until intrepid astronomers ventured far from Europe during the 16th and 17th centuries in order to chart the southern stars. Other new constellations were also proposed to fill gaps in Ptolemy’s classical sky map. Inevitably this led to arguments as astronomers disagreed.

>Much later, in 1922, things were finally put on a firm footing when the International Astronomical Union ratified 88 constellations with defined boundaries, mostly based on the Greek model. These were not the only aspects of Greek astronomy to pass into modern usage. There was one Ancient Greek in particular who was not prepared to tell stories about the stars, or speculate about them; he realized that the first step on the way to true understanding was to measure them. That man was Hipparchus and he defined a system of classification for the stars still in use today. The brightness of stars Even to a casual observer of the night sky, it is obvious that some stars are brighter than others. More than two millennia ago, Hipparchus meticulously compiled a catalogue of 850 stars, recording each star’s position and ranking its brightness. He had no equipment to measure the brightness; he simply made his estimates by eye. The brightest stars he called ‘first magnitude’, the faintest he termed ‘sixth magnitude’, and the rest he ranked in the categories between. Amazingly, astronomers still use this seemingly crude magnitude system today, although modern measuring devices have extended Hipparchus’s original six classifications. At the top end of the scale, the very brightest stars are now given negative numbers; at the other end, the stars that can only be seen with the aid of a telescope are assigned magnitudes with numbers often much higher than six. From the surface of the Earth, the best telescopes can detect stars of between 24th and 27th magnitude, but in orbit, above the distorting effects of the Earth’s atmosphere, the Hubble Space Telescope can detect 30th magnitude stars. Each magnitude category is about two and half times brighter than the previous one, so 30th magnitude is about 3.5 billion (3500 million) times fainter than the naked eye can see. But in measuring these perceived brightnesses, we are forgetting that the brightness of a luminous object is affected by its distance from the observer, as well by as the actual amount of light it gives out. Thus, a nearby dim star may well appear brighter than a highly luminous star far away. This behaviour is governed by what is known as an ‘inverse square law’, which means that if the distance doubles, the intensity of the light drops to a quarter; treble the distance and the intensity drops to one ninth of its original value. To acknowledge this, magnitudes measured without any correction for distance are known as ‘apparent’ magnitudes. The ‘absolute’ magnitude is the brightness value that has been corrected for distance. The red star Betelgeuse –widely known because it can be pronounced ‘beetle juice’ –has an apparent magnitude of 0.58, but leaps up to -5.14 on the absolute scale. It is a truly bright star indeed but comparatively far away. On the other hand, because it is so close, the Sun has an enormous apparent magnitude of -26.7, the brightest object in the sky. However, when corrected for its proximity, its absolute magnitude is just 4.8. In other words, our Sun, for all its glory and importance in driving life on Earth, is nothing but a thoroughly average star. Under the wandering stars The Ancient Greek astronomers, with their dedication to detailed observations and recordings, have left us a rich legacy of knowledge about the stars. The nature of five particular stars, however, eluded them. They called them planetes, meaning ‘wanderers’, because of their movement across the sky from one night to the next, unlike all other stars which remained ‘fixed’. From their Greek name you might correctly deduce that these planetes are in fact planets –our nearest five planets, Mercury, Venus, Mars, Jupiter and Saturn, which can be seen with the naked eye. The Greeks could have no concept that these were worlds in their own right, and imagined them to be gods, or at the very least emissaries of the gods, whose influence affected the fortunes of individuals on Earth. Two of these wandering planets, Mercury and Venus, follow orbits between Earth and the Sun. So, viewed from Earth they stay close to the Sun and are only ever seen in the twilight sky. Mars, Jupiter and Saturn orbit the Sun further out than the Earth and can be clearly seen making their slow paths through the night sky.

>With the advent of the Christian era, the opinion was widely adopted that the motion of the Heavens would always remain mysterious because the sky was God’s domain and mankind’s puny intellect could never understand His omnipotent will. This perspective began to change in the first decades of the 17th century when Johannes Kepler distilled the movement of the planets into three mathematical laws of planetary motion (see Why Do the Planets Stay in Orbit?). This proved that the Universe could not only be measured, but understood. At the same time, in Italy, Galileo Galilei was making discoveries that sparked our fascination with the wider Universe. In 1609, he raised his telescope and pointed it at the misty band of light that stretches across the night sky, known as the Milky Way. Through his basic telescope, tiny by today’s standards, Galileo could see that the Milky Way was composed of a multitude of faint stars. This was a revelation to all, because it had been believed that the entire Universe contained only what could be seen with the naked eye. Now, however, Galileo had shown that there was far more that lay beyond unaided vision. This realization was the start of the centuries-long fascination, with each generation of astronomers developing larger and larger telescopes to see fainter and fainter objects, which continues to this very day. The largest optical telescopes in use now are fully 10 metres across, some 500 times larger than Galileo’s original telescope.

>The thickness of the Milky Way’s stellar disc is estimated to be about 1000 light years, one light year being simply the distance that light travels in a year. According to laboratory measurements, light travels through a vacuum at approximately 300,000 kilometres every second (186 thousand miles per second), so in a year it travels about 9.5 trillion kilometres

>Near the centre of the galactic bulge, the density of stars is 500 times greater than in our neighbourhood. If the Sun and its family of planets were suddenly placed in the centre of the Galaxy, there would be other stars, possibly with their own planetary systems, just ten times further away than Pluto. In reality, within our solar neighbourhood the nearest star is more than 5000 times further away than Pluto. At the very centre of the Galaxy, astronomers believe that the density of matter is so great that a black hole exists

>As vast as it seems, the Galaxy is not the whole Universe; in the grand scheme of things, it is little more than a small island in an expansive ocean and there are innumerable other islands. Each one is a separate galaxy in its own right, containing anything from a few million to a trillion (a million million) stars.

>Although many galaxies seen are isolated, some of them are drawn together in clumps, attracted by one another’s pull of gravity. At the smallest end of this behaviour, a collection of less than 50 galaxies is simply known as a group. Our Galaxy is part of the Local Group, which contains one other large galaxy –a spiral called the Andromeda Galaxy –and around 30 smaller galaxies. What we call ‘clusters’ of galaxies are essentially large groups containing more than 50 galaxies and in some cases more than a thousand. The nearest clusters to the Local Group are the Virgo Cluster containing about 1300 galaxies, the Coma Cluster with over 1000 and the Hercules Cluster that has around 100 members.

>Equipped with his five senses, Man explores the Universe around him and calls the adventure Science.’ EDWIN HUBBLE 20TH CENTURY COSMOLOGIST

>Looking back in time To investigate the origin of these galaxies and thus uncover the evolution of the Universe, cosmologists exploit the fact that light does not travel instantaneously across space. As fast as the speed of light is by our everyday standards –light could circle Earth’s equator seven times in a second –it still takes many years to traverse the vast tracts of space between celestial objects. If a star is 100 light years away, its light takes 100 years to cross space to reach us and, as a consequence of this, we see it not as it is today but as it looked 100 years ago when the light began its journey.

>HOW BIG IS THE UNIVERSE?

>The cosmological distance ladder Imagine that the distance between the Sun and Pluto is the length of a soccer pitch. The Sun would be a globe just 2 centimetres in diameter. The Earth would be 2.3 metres away from the Sun and just 2 millimetres across. At the other end of the pitch, Pluto would be nothing more than a speck of dust. Where would the nearest star be? –In the crowd? –In the car park? –In the next street? All wrong. The nearest star would be 645 kilometres (401 miles) away. And that is nothing on the cosmic scale. Measuring the size of the Universe is unlike any other measuring job in human history. On Earth when you want to measure a distance, you can pace it out, or send a radar beam or laser ray to do the pacing for you. In the vast reaches of space this is usually impossible; the distances are simply too large. Bouncing laser beams or radar signals off celestial objects is only feasible for the Moon and the nearest planets. To measure other distances, astronomers have developed a filigree of different techniques for different distance ranges, called the ‘cosmological distance ladder’. The variety of approaches is essential because no single distance determination method can serve across all scales of the Universe. Some celestial objects are too faint to be seen far away, others are too rare to be found nearby. Where the techniques do cover the same range, they serve to reinforce each other and improve the overall accuracy of the system. Standard candles Central to the cosmological distance ladder is the concept of the standard candle. This is a type of celestial object that releases the same amount of energy regardless of where in space it is found. Its distance is therefore the only thing that affects how bright it appears from Earth. One of the best types of standard candle is the so-called Cepheid variable star. The first example of such a star to be observed, Delta Cephei, caught a young astronomer’s attention in 1784. John Goodricke ofYork charted the way it rose in brightness and faded again, deducing that the entire cycle took 128 hours and 45 minutes to complete. At this time astronomers knew of a handful of other variable stars, but each of those dropped in brightness suddenly and then restored themselves some time later. Delta Cephei was the only one to exhibit a gradual change.

>‘Measure what is measurable, and make measurable what is not so.’ GALILEO GALILEI 17TH CENTURY ASTRONOMER

>By the first decade of the 20th century, many more examples of Cepheids had been discovered, some with shorter pulsation periods, some with longer. Henrietta Swan Leavitt, an assistant at the Harvard College Observatory, compiled a list of Cepheids in a nearby galaxy called the Small Magellanic Cloud. Rather than list them in random order, she wrote the 16 entries out according to how long they took to pulsate. Her curiosity was piqued by the fact that listed in this way the stars also appeared in order of average brightness: the longer the pulsation period, the brighter the star. By 1912, Leavitt had investigated a further nine Cepheids in the Small Magellanic Cloud and confirmed that each one’s period of pulsation was tied to its average brightness. This immediately suggested that the Cepheids could be used as standard candles. Final confirmation of this was supplied by British astrophysicist Arthur Stanley Eddington when he explained the behaviour of Cepheid variables. He proposed that the surface of the star trapped some of the outflowing radiation, causing the surface to swell up before releasing that radiation and shrinking again. What was more, he showed that the density of the star determined the period of pulsation. This means that all Cepheids pulsating with a period of say five and a half days will be identical to one another. Thus, any two can be compared and the difference in their observed average brightness can be used to calculate how much further one is than the other. The comparison of Cepheid variable stars from one galaxy to another is one of the most heavily relied upon methods in distance determination. A brighter standard candle is an exploding star, the supernova type Ia. The explosion is triggered when the burnt-out core of a dead star siphons gas from a nearby, active star. The gas builds up on the surface of the dead star, increasing its mass. When this crosses a well-defined threshold called the Chandrasekhar limit (after Subrahmanyan Chandrasekhar, the Indian physicist who computed its value), the star can no longer support its own weight and collapses, setting off an enormous explosion. Because every supernova of this type is caused by the collapse of the central star when it reaches this mass limit, every one of these stellar cataclysms releases the same amount of energy into space. The brightness of a supernova type Ia is vastly greater than that of a Cepheid variable star; in fact it can outshine 100 billion (100,000 million) normal stars put together, rendering it visible across the entire Universe. The drawback is that these celestial detonations are impossible to predict because astronomers cannot see the doomed stars until they explode. Statistically, a supernova will occur about once a century in any given galaxy and this makes spotting one a task that requires constant vigilance. Once a supernova type Ia has been seen, the distance of its host galaxy can be calculated relative to any other galaxy that has displayed a similar supernova. The same is true of galaxies seen to contain Cepheid variable stars. So, to join these two rungs of the cosmic distance ladder together, astronomers need to see a supernova in the same galaxy as they have seen a Cepheid. The more galaxies they can do this for, the better the accuracy of the join. Both methods give only relative distances, telling us for example that one celestial object is ten times farther away than another. What astronomers really want to know is the actual distance in kilometres or miles. To do this, they need to put a solid first rung on the cosmological distance ladder –one that supplies absolute distances. Luckily there is a method to do this and it is called ‘parallax’.

>PARALLAX: THE MOST ACCURATE FORM OF DISTANCE DETERMINATION

>Bessel’s parallax measurement was swiftly followed by results for other stars from other astronomers. Despite this triumph, the work was painstaking and prone to error. By the first decades of the 20th century, astronomers had measured only about 100 stellar parallaxes. Today, satellite measurements have supplied parallaxes for over 100,000 stars, but all well within the Milky Way at distances of less than 1000 light years. Fortunately, some of these stars are Cepheid variables, so this has allowed the absolute brightness of Cepheids to be calibrated and the bottom rung of the distance ladder to be joined to the rest.

>Expanding space In 1916, several years before Hubble undertook this work, Albert Einstein presented his General Theory of Relativity to the world. General relativity provided the mathematical framework within which to properly understand the meaning of Hubble’s subsequent observations. It too proposed that the galaxies were separating from one another, but rather than individual galaxies speeding through space, its premise was that space itself was expanding, carrying the galaxies along with it. A useful way to visualize this is by thinking of a raisin cake mixture. Before rising in the heat of the oven, it is a small dense lump with closely packed raisins. After it has risen, the raisins have not moved through the mixture but they have been driven apart by the expansion of the cake. So it is with the galaxies; space itself is endowed with the ability to expand and, as it does so, it drives the galaxies apart. The more space between a pair of galaxies, the faster they are driven apart and the greater the redshift. When astronomers talk about a redshift of 1, they mean that the light has been doubled in wavelength. To do this, the Universe must have doubled in size while the light has been travelling along. A redshift of 3 means the light’s wavelength has quadrupled, hence the Universe has doubled in size twice, meaning it is now four times as big as when the light started its journey.

>‘The history of astronomy is a history of receding horizons.’ EDWIN HUBBLE 20TH CENTURY ASTRONOMER

>Getting back to our aim of measuring cosmological distances –if we know a galaxy’s redshift, we can convert it to a distance once we know how fast the Universe is expanding. The expansion rate of the Universe is known as the ‘Hubble constant’, but measuring it confounded astronomers for most of the 20th century. Hubble himself got it wrong by a factor of almost seven. Although the 100-inch telescope he was using was then the largest in the world, it was incapable of resolving Cepheids throughout his sample of galaxies. So, although Hubble showed us how to measure the size of the Universe, the technological restrictions of his day meant that he could not complete the job –not even when he started using the 200-inch Hale telescope on Mount Palomar, California. In the last two decades, astronomers have had another Hubble –the Hubble Space Telescope –which has allowed them to finish the task, thanks to its vantage point above Earth’s atmosphere. Although Hubble’s mirror is only about half the size of the Hale Telescope, it has identified Cepheids out to around 60 million light years and supernovae type Ia across billions of light years. It has enabled astronomers to calculate a definitive figure for the Hubble constant, which tells us that for every million light years that separates two galaxies, they are driven further apart at a speed of 22 kilometres per second (14 miles per second). How big is space? Using the now well-constructed cosmological distance ladder, astronomers today are confident that the Universe stretches on for billions of light years in all directions. The most distant celestial object yet observed has a redshift of just over 8. This means that the Universe has doubled in size eight times during the time it has taken for the light to reach us. It converts into a distance of around 13 billion light years, and this is our current estimate for the minimum possible size of the Universe. It has taken the light from that celestial object 13 billion years to cross the Universe and in all that time the Universe has continued to expand. To calculate the full extent of the Universe, the only recourse is to create computer models of the expanding Universe based upon the laws of general relativity. These suggest that during those 13 billion years, the Universe has swollen to at least 95 billion light years in diameter. As if that were not brain-bending enough, there is one final sting in this tale. The Universe may extend way beyond 95 billion light years, but there is no way for us to see beyond even 13 billion years because there has not been enough time since the origin of the Universe for the light from such distant regions to reach us. If we wait another billion years, then the light from the next billion light years of space will arrive and we will be able to study it. Like a television soap opera, the study of the Universe seems destined to be a never-ending series of revelations.

>How Old is the Universe?

>Man is slightly nearer to the atom than to the star . . . From his central position man can survey the grandest works of Nature with the astronomer, or the minutest works with the physicist.’ ARTHUR EDDINGTON 20TH CENTURY ASTROPHYSICIST

>How Did the Earth Form?

>The Earth and the rest of the Solar System formed during a hellish maelstrom around 4.6 billion years ago. Such activity is driven by gravity and continues in other parts of the Universe today, forming new stars and planets, but it is all hidden away inside dusty cocoons that absorb ordinary wavelengths of light. To peer inside these celestial construction sites, astronomers have to use other wavelengths, such as infrared or microwaves. The goal of this research is simple: to see what is happening in these regions of activity and relate it to what must have occurred during the formation of our own Solar System. By doing this, the astronomers hope to solve a number of mysteries. The first of these unknowns is why the planets of the Solar System all orbit in more or less the same plane. Presumably, there must have been some natural process that prevented the planets from assuming random orientations and corralled them into nested orbits. The second is, why there is such an abrupt change in the composition of the planets from the inner to the outer Solar System. Some process segregated the raw material to produce relatively small rocky worlds in the inner Solar System and giant gaseous planets further out. To answer these questions, astronomers must identify and observe sites of star and planet formation elsewhere in the Galaxy.

>Giant molecular clouds Stars and planets account for 85 percent of all the atoms in the Galaxy, leaving around 15 percent available in interstellar regions where new stars and planets form. This material floats through space creating a tenuous mist of gas and dust, of which hydrogen is the most abundant constituent, and the ‘dust’ consists of particles of heavier elements from planetary nebulae and supernovae (see What Are Stars Made From?). In space there are typically only 100,000 hydrogen atoms in every cubic metre (at sea level on Earth the atmosphere contains around 200 million trillion atoms). While floating through the Galaxy, the hydrogen atoms tend to pair up into molecules, and the molecules gather together into enormous clouds. Astronomers estimate that there are about 4000 of these giant molecular clouds in the Milky Way Galaxy. Each one is typically between 150 and 250 light years in diameter and contains enough gas to make up to ten million stars like the Sun. As the molecular gas falls together it sets the cloud rotating, a fact that will become very important for the subsequent formation of stars and planets.

>Solving the mysteries Planet-forming discs are flat because of centrifugal force. This is not a force based upon an intrinsic property, such as electrical charge, but is instead created when an object begins to spin. The faster something rotates, the greater the centrifugal force it generates, pushing objects outwards: so a ball placed on a merry-go-round will roll towards the outer edge as the merry-go-round begins to spin. It is also the reason a car will leave the road if the driver takes a bend too fast. Consider the clump of gas and dust that formed our Solar System. As the spherical clump contracted under its own gravity, it naturally spun faster, in the same way that a spinning ice dancer speeds up when she pulls in her arms. This would have boosted the centrifugal force felt in the equatorial plane of the sphere, opposing the pull of gravity on the gas and dust and slowing the collapse in this region. Away from the equator, where the effects of centrifugal force were not so great, the gas and dust would have fallen inwards more or less freely, so the overall result was that the cloud pancaked into a flat disc. Within this disc the planets subsequently materialized, and therefore they occupy almost identical orbital planes rather than whizzing around the Sun in random orientations. This provides the solution to the first mystery. The second mystery was why the composition of our Solar System planets changes so drastically from the inner regions to the outer regions. In the inner Solar System, the planets are like the Earth –small, rocky, and with thin atmospheres –whereas in the outer Solar System they are more like Jupiter –large, gaseous, and with thick atmospheres. This difference comes about because, as the newly forming Sun shrank to its modern size and density, it released energy; nowhere near as much energy as would be released later when nuclear fusion ignited in its core, but nevertheless enough to heat the surrounding disc. The heat prevented certain chemicals from forming by driving particular atoms into frantic motion. Other atoms and molecules were not so affected by the heat. They bumped into one another and bound together, creating dust of different chemical compositions. The chemicals that formed depended on the temperature, which in turn depended on the distance from the forming Sun. Near the Sun, in what would eventually be Mercury’s orbit, the temperature would have been several thousands of degrees and only metallic and some silicate atoms could condense into dust (hence Mercury’s metallic core takes up a large proportion of is volume); other chemicals would have been vaporized back into gas by the heat from the young Sun. In the somewhat lower temperatures near the Earth’s eventual location, more silicates would have been able to condense (giving us a smaller metallic core in comparison with the rest of our planet). Further out still, the lower temperatures enabled other elements to form dust.

>The snow line A distinct boundary in the planet-forming disc occurred at five times the Earth’s distance from the Sun. Called the snow line, it is where Jupiter orbits today. At the snow line, when the planetary material was condensing, the temperature would have been around 90 K, low enough for molecules such as water, ammonia and methane to form ice. (Astronomers measure temperature in kelvin, K. Zero on the kelvin scale is the temperature at which atoms and molecules all cease to transfer energy between one another, known as ‘absolute zero’, this is the equivalent of approximately –273 degrees Celsius.) At 40 times the Earth’s distance from the Sun, roughly where Pluto orbits, the temperature would have been just 20 K and almost every chemical element could condense; the only exceptions were hydrogen and helium, which remained in a gaseous state. Consequently the dust grains developed different compositions throughout the planet-forming disc, leading to the variety of planets. The much larger size of the outer planets can be explained by the much greater reservoir of matter beyond the snowline. In the inner Solar System, the dust gradually accumulated into objects resembling small asteroids; these ‘planetesimals’ would have populated the early Solar System in vast numbers. To build Mercury, Venus, Earth and Mars would have required ten billion or more planetesimals of 10 kilometres (6 miles) in diameter. These lumps of rock continued to grow by basically colliding and sticking together, but the process was subtler than it may at first sound. Head-on collisions were no use because they released too much energy and would have shattered the planetesimals, blasting the debris into space. In any case such energetic clashes would have been rare because the planetesimals were all rotating in the same direction. Sometimes an impact was just enough to melt the planetesimals together, at other times although it broke them into fragments, the pieces remained together and continued to orbit as a pile of rubble. These close encounters were repeated along entirely random lines until eventually some larger planetesimals began generating enough gravity to pull smaller ones onto themselves. Throughout the disc, these major planetesimals began to outpace their lesser companions and, the bigger they became, the more efficient they grew at drawing in smaller bodies. Astronomers call these planetesimals ‘oligarchs’, because they controlled their surroundings. Essentially they were small rocky planets, each containing between the mass of the Moon and Mars; computer simulations show that 20 to 30 of them must have ultimately smashed together to build the Solar System’s four terrestrial planets of today.

>Gas giants and planets beyond The Solar System’s gas giants –Jupiter, Saturn, Uranus and Neptune –were probably formed in a similar way to the inner planets but from bigger oligarchs, bolstered by the astronomical ices. Once the forming Jupiter and Saturn reached between three and five times the mass of the Earth, they generated so much gravity that they began pulling in gas from their surroundings and this gave them their thick atmospheres that mirror the cosmic abundance of elements. Uranus and Neptune formed in a similar fashion although, being less massive, they were not so good at attracting the hydrogen and helium, so they display a greater proportion of astronomical ices in their atmospheres. A number of astronomers have put forward the alternative suggestion that the gas giants formed in the same way as stars do. In this scenario, a region of the disc around the Sun would have reached a critical density and gravity simply pulled it all together. There were no oligarchs building up and colliding, just a sudden collapse of gaseous matter into a giant planet. At present there is no way to determine which of these gas giant formation scenarios actually took place in the Solar System. Both theories correctly predict that the giant planets surround themselves with their own mini-discs, which subsequently coalesce into extensive moon systems. At the distance of Pluto, the density of matter orbiting the young Sun was thinner, so the bodies that formed there were consequently smaller. Pluto itself is only two-thirds the size of Earth’s Moon, and has an orbital plane significantly inclined to that of the other planets. In 2006, these factors, together with the discovery of a number of other Pluto-like objects in the outer reaches of the Solar System, led to the International Astronomical Union voting to downgrade Pluto from the status of planet to dwarf planet. The newly observed bodies included the icy celestial objects Haumea and Makemake, but it was the body catalogued as 2003 UB313 that really triggered the debate. Observations showed that it was at least the size of Pluto, probably bigger. Hence, astronomers faced a choice: downgrade Pluto or name 2003 UB313 as the tenth planet in the Solar System. A heated discussion ensued, during which 2003 UB313 was nicknamed Xena, after the television heroine Xena: Warrior Princess. Eventually, the decision was made: 2003 UB313 was not a planet and as a result Pluto was downgraded. Xena’s name was changed to Eris –rather appropriately –after the Greek goddess of strife and discord.

>There are undoubtedly many more dwarf planets yet to be found in the Solar System. Astronomers estimate that there could be hundreds or even thousands of them beyond Pluto, and possibly a few fully-fledged planets. In fact, according to computer simulations, a whole second Solar System’s worth of planets may be lurking at thousands of times the distance of Earth from the Sun. These could turn out to be as big as Mars or even Earth –not formed in situ, but thrown there by the gravity of the gas giant planets. If a planetesimal were travelling sufficiently fast near a giant planet, the giant’s gravity would be unable to pull it into a collision. As the smaller object sped by, the near miss could result in it gaining considerable speed and being boosted into a larger orbit. In this way, Jupiter could have scattered rocky planets out to between 25 and 250 times further from the Sun than Pluto. At such a distance from the Sun, the scattered planets would be very faint indeed and so extremely difficult to spot. Add to this that Jupiter could have catapulted them into any orbital plane, and the only way astronomers will be able to search for them is to trawl the whole sky with a powerful telescope. There are a number of such instruments on the drawing board at the moment, all due to begin searching within ten years. The heavy bombardment By 4.6 billion years ago the Solar System looked almost as it does now; the familiar planets and their moons had formed. The space between the planets, however, remained home to countless, smaller leftovers. These tiny objects ranged from pebbles and rocks to planetesimals that had so far escaped the planets’ gravitational clutches. Jupiter’s gravity trapped many of them between itself and Mars to form the asteroid belt, but most whizzed about the Solar System. During the next 700 million years these celestial vagabonds collided with the planets and their moons, blasting out craters of all sizes. Old planetary surfaces are easily identified today because of their heavily pockmarked appearance: our Moon being the classic example. Its scarred face has taught astronomers much of what they know about this last phase of planet formation, referred to as the ‘heavy bombardment’ period. On Earth, the early craters have been eroded away; today, less than 200 craters are known, and all of them are from comparatively recent impacts. The position of the Earth inside the snowline meant that it was formed without any water; the heat from the young Sun would have vaporized any water molecules that formed. This is another puzzle that astronomers have had to address: how we came to have oceans. The ‘late bombardment’ suggests a way; planetesimals that formed in the outer Solar System, incorporating water and other ices, rained down on Earth and the other planets of the inner Solar System, supplying them with water and other volatile substances that they lacked. On Earth this material was swiftly transformed into life (see Are We Made from Stardust?). During the bombardment period, many of the planetesimal remnants would have been ejected from the solar vicinity by near misses with Jupiter, in the same way that Jupiter is thought to have scattered planets. Because they were much smaller than planets, Jupiter could have lofted them much further, throwing a trillion or more of them into incredibly large orbits, reaching 10,000 to 100,000 times the Earth’s distance from the Sun. This distant collection is called the ‘Oort Cloud’. Occasionally one of its members returns to the inner Solar System, and we call these distant visitors ‘comets’. Being composed of ice they begin to melt when they approach the Sun, and leave a trail of gases in space that we see illuminated as the comet’s tail. Dust released from the melting ice litters interplanetary space, and if the dust falls into the Earth’s atmosphere it burns up and creates meteor showers, or shooting stars. Now and then, if a conglomerate of dust is large enough, it might not burn up entirely but plummet all the way to strike the ground as a meteorite. Fragments chipped from asteroids can also fall as meteorites.

>Why Do the Planets Stay in Orbit?

>The second law he deduced is that, as a celestial object follows its orbit, it sweeps out equal areas of the ellipse in equal times. To visualize this, imagine a long extensible tether between a planet and the Sun. As the planet moves a small distance through its orbit, so the tether pivots at the Sun, sweeping out a triangular area. When the planet is far away from the Sun, the tether is longer and the planet does not have to move far to sweep out a large area. Conversely, when the planet is close to the Sun, the tether is much shorter and the planet has to move much faster to sweep out the equivalent area in the same time. Hence, what the law is saying is that when a planet is far from the Sun it travels slowly and when it is closer to the Sun it travels faster. This was an important clue because it implied that whatever force was moving the planet weakened with distance. ‘If there is anything that can bind the heavenly mind of man to this dreary exile of our earthly home and can reconcile us with our fate so that one can enjoy living, then it is verily the enjoyment of the mathematical sciences and astronomy.’ JOHANNES KEPLER 17TH CENTURY ASTRONOMER

>When a celestial object follows a closed orbit in space, where there is no air resistance, it circulates time and time again, and ellipses and circles are the observed orbital shapes for the planets, moons and asteroids in the Solar System. Some comets, such as Halley’s famous one that we see every 76 years, also follow closed orbits. Stars follow closed orbits around the centre of their galaxy and even the mighty galaxies follow closed orbits around the centres of galaxy clusters.

>Unsurprisingly, Newton’s work was heralded as ‘the system of the world’; a phrase for what we would now refer to as ‘the theory of everything’. Over the course of the next few centuries, scientists came to realize how much else there was still to understand in the physical world: electricity and magnetism, nuclear forces, and relativity and quantum effects. But at the time, Newton’s work was a triumph, and one of its victories was providing the explanation of the tides. The tidal changes were an all-important phenomenon for a sea-faring nation at that time, but their cause was a mystery until Newton proved in the Principia that they were due to the gravitational attraction of the Moon and the Sun on the oceans.

>Was Einstein Right?

>The fabric of space Einstein’s great insight was the concept of the ‘space–time continuum’. This is a fabric –for want of a better word –that stretches through all space, in all directions, and includes time as the fourth dimension. In Newton’s theory of gravity, both space and time were imagined to be rigid frameworks, absolute and invariant; they were also quite separate concepts. In general relativity, space and time constitute a flexible continuum that can be stretched and warped by the presence of matter and energy. The warping of space–time can affect time as well as space, leading to a number of brain-bending consequences such as time dilation (see Can We Travel Through Time and Space?). Einstein used the idea of the space–time continuum to explain gravity, stating that it was an effect created by the warping of space–time in the presence of matter. Think of a suspended sheet of rubber onto which is placed a heavy object. The object warps the rubber sheet, forming a curved depression. Any smaller object that is placed nearby will roll down the depression, spiral around the heavy object and eventually collide with it. This is analogous to the way in which gravity acts. In this scenario, the two-dimensional rubber sheet is the equivalent of the four-dimensional space–time continuum.

>Einstein’s lift Imagine that you are in a lift, totally enclosed with no windows. When the lift is stationary, at whatever storey, gravity pulls you down as you stand on the floor. If someone were to cut the cables, the lift would fall and you would suddenly feel weightless. Any tiny body movement would result in you floating away from the floor because you would be falling at the same speed as the lift. This is how astronauts feel when they are in orbit. Inside the lift, you would continue to float until you hit the ground and your experiment came to a gruesome conclusion. Now take the same lift into outer space, well away from any gravitating object. This time, you feel weightless because there are no gravitational forces acting on you and you are floating through space at the same speed as the lift. This is entirely equivalent to the lift falling on Earth. If you were trapped inside, you would be unable to distinguish between the two cases. Finally, imagine strapping a rocket motor to the base of the lift in space and turning it on. The lift accelerates but your inertia does not want you to move, so you drop to the floor of the lift, which then pushes against you to accelerate you to the same speed as the lift. To you, this feels like the force of gravity experienced when you are standing in the lift on Earth; the faster the acceleration, the greater the force you feel –it is like experiencing a very great gravitational force. Indeed, in fastmoving jets and rockets, this force is often called the ‘G-force’. The principle of equivalence encapsulates the results of the lift ‘thought experiment’ by stating that acceleration is indistinguishable from a gravitational field. Once Einstein had accepted this, general relativity fell into place. His predictions are identical to those of Newtonian gravity when the gravitational force is weak or, in the language of relativity, when the curvature of space–time is shallow. However, when gravity becomes stronger and the curvature becomes more pronounced, general relativity predicts corrections to the way gravity acts on celestial objects. This is how Einstein explained Mercury’s wayward orbit. Unlike the other planets, Mercury is close enough to the massive Sun for the curvature of space to be an important factor and so general relativity was needed to correctly calculate its movement. It was an early success for Einstein’s theory, but not the final proof. As important as resolving a known problem is for a theory’s credibility, the real test is whether it can predict something totally new. Einstein did not fail in this, either. He saw that, according to his theory, gravity should bend starlight much more than predicted by Newton’s theory. The difficulty was, how he could prove it. Gravitational lensing To Newton, gravity could only affect objects with mass and, since light was a massless ray of energy, he considered it should pass through a gravitational field relatively unaffected. However, Einstein predicted that light would have to follow the contours of space–time, just as certainly as did planets and moons. His calculations showed that, when passing through a gravitational field, light would be deflected slightly from its path, in the same way a golf ball is deflected if it just clips the hole. The concept is called ‘gravitational lensing’, and further calculations showed that the only object in the Solar System capable of bending starlight by a measurable amount is the Sun. The only time astronomers can see stars close to the Sun is during a total eclipse when the glare is blocked by the Moon. In 1919, Arthur Eddington led an observing party to the African island of Principe in the Atlantic Ocean to take the necessary measurements during the upcoming solar eclipse. Although cloudy in the morning, the weather cleared up in time for the dramatic total phase of the eclipse. In the sudden darkness, Eddington took his photographs, then measured the positions of the stars on the developed plates and compared them to photographs taken when the Sun was nowhere near the same stars. He found that the stars had apparently moved from their expected positions, just as Einstein had said they would if light were deflected by gravity.

>GRAVITATIONAL LENSING: STARLIGHT IS DEFLECTED BY THE GRAVITY OF THE SUN

>‘Oh leave the Wise our measures to collate, One thing at least is certain, light has weight. One thing is certain and the rest debate, Light rays, when near the Sun, do not go straight.’ ARTHUR EDDINGTON 20TH CENTURY ASTROPHYSICIST The conclusion was inescapable: general relativity was correct. As the news spread around the world Einstein became a superstar, but this tremendous success has come at a price to modern physics: we cannot yet, almost a century later, integrate general relativity’s explanation of gravity into our description of the other forces of nature. If we could, we might at last be able to establish the so-called ‘theory of everything’. Is gravity a force after all? If we include gravity, there are four fundamental forces of nature. First there is the familiar force of electromagnetism; it is responsible for all the electrical and magnetic phenomena that we know of, and has been well-harnessed by science and technology since the late 19th century. Two other fundamental forces came to light in the early 20th century as physicists succeeded in probing the nucleus of the atom. These two nuclear forces are known, rather unimaginatively, as the ‘strong nuclear force’ and the ‘weak nuclear force’. The strong nuclear force holds an atomic nucleus together and must be overcome in order to split an atom or to fuse two together. Hence, it is the strong nuclear force that allows energy to be liberated by stars and from atom bombs. The weak nuclear force governs certain forms of radioactive decay. Taken as a force, gravity would be the weakest of all the fundamental forces. An easy way to demonstrate this is to watch an iron nail leap up towards a handheld magnet, proving that the magnetic force generated by the magnet in your hand is able to overwhelm the gravitational force created by the entire Earth. Nevertheless, it is the force of gravity that sculpts the Universe on its largest scales because it acts over vast distances, whereas the other forces are limited in the range over which they act. The two nuclear forces are confined to the width of an atomic nucleus; the electromagnetic force, although longer in range, tends to cancel out over large distances because it has both positive and negative charges, giving rise to repulsion (between like charges) as well as attraction (between opposite charges). Physicists can explain the force fields associated with electromagnetism and the two nuclear forces as an exchange of tiny short-lived particles, called virtual particles. Gravity, on the other hand, can only be explained as a large-scale curvature of space. Many physicists suspect, and hope, for the symmetry and completeness of their theory, that gravity will eventually be found to be a real fourth force field, exchanging tiny virtual particles called gravitons. If that proves to be so, Einstein’s curvature of space will turn out to be a scientific metaphor, useful for visualizing gravity before the true explanation was found.

>Little green men Einstein himself thought that his theory was likely to fail in extremely strong gravitational fields; in the 1950s, a class of celestial object was found that generated just such a strong gravitational field. That era saw the burgeoning use of radio telescopes, and a Cambridge graduate student, Jocelyn Bell, discovered a pulsating radio signal that was clearly celestial in origin because it appeared in exactly the same place in the sky night after night. It pulsed on and off, as regular as clockwork. She called the source LGM-1, which stood only half-jokingly for Little Green Men. Before long, however, the theoreticians showed that it was most likely to be the spinning super-dense remnant of an exploded star. Such a neutron star is even smaller than its cousin the white dwarf (see What Are Stars Made From?). Whereas white dwarfs are about the size of the Earth and hold as much mass as the Sun, a neutron star is the size of a small asteroid –just 10 to 20 kilometres (6 to 12 miles) in diameter –and contains several times the mass of the Sun. The whole neutron star is packed with matter as tightly as in an atomic nucleus, hence its enormous density and very strong gravitational field. Radio astronomers tend to call them ‘pulsars’ because, as a neutron star spins, it can sweep powerful beams of radio emission through space, and, like a lighthouse beam, the radio signal appears to pulsate on and off as it sweeps over the Earth.

>What is a Black Hole?

>Eventually the natural philosophers decided that Newton’s laws precluded light from being affected by a gravitational field and so light would always leave a celestial object, no matter how strong its gravity. The matter rested for a couple of centuries until Einstein published his General Theory of Relativity in 1915 and showed that gravity did indeed affect light (see Was Einstein Right?). Less than two months after Einstein’s publication, German mathematician Karl Schwarzschild found that Einstein’s equations allowed celestial objects to become so dense that they create gravitational traps. The size of each trap, known as its ‘Schwarzschild radius’, is determined by the mass inside. For example, a black hole containing the mass of the Earth would have a Schwarzschild radius the size of a small coin, whereas a black hole containing billions of times more mass than the Sun would be as large as our Solar System. Once any object, or even light, had passed beyond this Schwarzschild radius –or ‘event horizon’ –it could never escape. Astronomers were forced to accept that black holes could exist, but the dilemma was how they could possibly observe something that emitted no light or any other type of radiation. The solution did not come until the early 1970s, when the first X-ray telescopes were lofted into space and revealed an extraordinarily bright X-ray source in the constellation of Cygnus, some 8000 light years away. After much analysis, it was decided that the source of X-rays was a superheated cloud of gas spiralling into a black hole, dubbed Cygnus X-1. As the gas accelerated in the immensely strong gravitational field outside the event horizon, it was heated to millions of degrees and began to emit X-rays. Since then numerous observations have been made of a black hole ‘feeding’, for example by ripping apart a companion star. The blue supergiant star HDE 226868 is about 30 times more massive than the Sun and 400,000 times brighter. The black hole next to it contains only between 5 to 10 times the mass of the Sun but has a gravitational field so strong that it has pulled the blue supergiant into an egg shape and is now greedily stripping it of gas. The gas falls from the giant star and enters a brief spiral orbit around the black hole, forming what astronomers call an accretion disc. In the maelstrom, magnetic fields compress some of the gas into jets that seem to shoot away from oblivion. Most of the gas, however, ends up heading into the black hole like water spiralling down a plug hole and then disappears forever. Small, intermediate and supermassive black holes Black holes such as Cygnus X-1 are known as ‘stellar’ black holes. They contain several times the mass of the Sun and are formed when very massive stars explode as supernovae (see What Are Stars Made From?). The supernova is triggered when the inert iron core of the star collapses to become a neutron star. As the outer layers of the star come crashing down, igniting the supernova explosion, the neutron star at the core is pummelled and some of the outer material is absorbed, increasing the core’s mass, which can increase the gravitational field so much that the core becomes a black hole. Whilst such stellar black holes are the most prevalent kind, there are now known to be other, larger, black holes with different formation mechanisms. The next size up is termed an ‘intermediate mass’ black hole. In common with the stellar black holes these orbit the centre of their galaxy and contain a few hundred or a few thousand solar masses. Astronomers are not sure how these form; possibly they result from several stellar-sized black holes merging together.

>ANATOMY OF A BLACK HOLE: A BLACK HOLE IS NOT A SIMPLE OBJECT BUT IS MADE UP OF SEVERAL COMPONENTS

>Thirdly, there are the ‘supermassive’ black holes, containing anything from millions to billions of times the mass of the Sun. A supermassive black hole is thought to sit at the centre of every galaxy, but taking up no more volume than an average solar system. In 90 percent of galaxies the central supermassive black hole is inactive but, in the other ten percent, it is constantly feeding from surrounding celestial objects and this drives an extraordinary engine of activity that can be seen across billions of light years of space. Active galaxies Vast quantities of radiation are released by an active galaxy, all derived from matter heating up before it plunges into the supermassive black hole at the galaxy’s centre. The most powerful active galaxies generate more energy per second than a trillion Suns, with the result that the active nucleus outshines the rest of the galaxy by a hundred times or more. This brilliance masked the nature of an active galaxy for some time; when astronomers caught their first glimpses of them during the 1950s, they saw the star-like active cores and assumed that they were peculiar stars in our own Galaxy. They called them ‘quasi-stellar’ objects, from which the present name quasar is derived.

>‘[The black hole] teaches us that space can be crumpled like a piece of paper into an infinitesimal dot, that time can be extinguished like a blown-out flame, and that the laws of physics that we regard as ‘sacred’, as immutable, are anything but.’ JOHN WHEELER 20TH CENTURY PHYSICIST

>The true identity of quasars was revealed in 1962, when astronomers discovered that they are in fact incredibly distant, so could not be stars but had to be intensely powerful galaxies. As the distribution of these highly active galaxies was charted, it was revealed that all quasars are in the far reaches of the Universe and none exist nearby. Because light takes so long to travel across space this means that quasars are ancient objects –they seem to have populated the Universe in their greatest abundance around 10 billion years ago. This leads astronomers to conclude that quasars are a phase that every galaxy passes through, brought about when the supermassive black hole at its centre has a lot of matter to consume. Less powerful active galaxies can be found throughout the Universe at all distances. Some may be ageing quasars whose food source is almost used up. When the supermassive black hole finally devours everything within its reach, the active galaxy quietens to become a normal galaxy, such as our own. But there is nothing to stop the black hole coming back to life if more matter falls into its clutches. According to calculations, one medium-sized star like the Sun wandering too close to the galactic centre is all that would be needed to re-ignite the activity and keep the black hole spewing energy for a year. So, the current population of active galaxies must be transient. If we came back in a million years’ time, some presently active galaxies would have become inactive, whilst other currently quiet ones would be blazing with energy.

>Evaporating black holes There may be a fourth type of black hole, at the opposite end of the scale. These are tiny ‘primordial’ black holes, thought to have been created during the Big Bang when the space–time continuum was so crushed that minuscule regions could seal themselves off from the rest of the cosmos. Such a primordial black hole has never been detected although, in the 1970s, the celebrated physicist Stephen Hawking suggested a way in which we might see them, and at the same time prove that black holes are not completely black. He did so using another cornerstone of modern physics: quantum theory. First propounded in the early decades of the 20th century, quantum theory describes the Universe on its smallest scales. Central to its explanation is that energy comes in discrete packets, so what may look like a beam of light is actually made up of a multitude of tiny particles called photons. Each one of these photons carries a well-defined amount of energy; for example, a photon of blue light carries twice the energy of a photon of red light. Quantum theory also tells us how particles behave, and one of its tenets is that a particle is difficult to pin down to a specific place. Physicists can calculate where they expect a particle to be, but it could be just as easily somewhere else in a small region around this calculated position. This means that particles travelling close to a boundary can sometimes appear to have spontaneously jumped across it, a phenomenon known as ‘tunnelling’. It is manifest in actual observations: it makes fusion in stars possible at temperatures lower than would normally be needed, because the closely packed atomic nuclei occasionally find themselves close enough to their neighbour to fuse together and release energy. According to Hawking, a particle can tunnel from the interior of a rotating black hole and escape, thus lowering the black hole’s mass. As the black hole loses mass, so the process continues and speeds up, until the black hole disappears in a sudden release of gamma rays. The most likely black holes that ‘evaporate’ in this way are the primordial ones, because they will be evaporating faster than they are capable of consuming. Yet, although gamma ray satellites have been on the lookout for this behaviour, nothing has yet been seen. It is possible that the Large Hadron Collider in Switzerland will create minute black holes in particle collision processes, which may evaporate in a fraction of a second and give scientists their first glimpse of this process. The problem of the singularity Although black holes are now an accepted part of the pantheon of celestial objects, there is still unease about them in astronomical circles. One of the uncanny things is that mathematically they share a striking similarity to the Big Bang. At first, there may not seem to be anything in common between a black hole, which sucks things out of existence, and the Big Bang, which created the Universe and set it expanding. However, to a mathematician they share an identical feature: a point of infinite density and zero volume, known as a ‘singularity’. Inside a black hole, the singularity is presumed to be the last resting place of matter because gravity crushes that matter into smaller and smaller volumes. This presents a problem for physicists because as the volume approaches zero no theory can be used to study the resulting singularity. There is also something known as the black hole ‘information loss’ problem. To the outside Universe, only three properties of the black hole are visible: its mass, its electric charge and its angular momentum (rotation). Any other information, such as what fell in, appears to be lost. This goes against the grain of one of the deepest principles of physics: that of reversibility. If you drop something into a bowl of water and let it dissolve, in principle someone could analyse the water, and identify what has been dropped in there. They could even separate the substances again by boiling off the water and thus reconstruct the original material. This is reversibility. In the case of a black hole, once something crosses the event horizon we cannot then discover what it was, let alone recover it. All the stars and planets that have been devoured have been erased from the Universe: their composition, their temperature, their density –all are gone. Not even particles tunnelling out, according to Hawking, can provide us with the missing information.

>How Did the Universe Form?

>The beginning of everything Current physics cannot describe the very beginning of the Big Bang, because it cannot deal with the tiny fractions of time and space that would need to be considered. The smallest unit of time that physics can presently handle is 10-43 seconds: a decimal point followed by 42 zeros and a one. It is known as the ‘Planck time’, after Max Planck, the father of quantum theory (see What is a Black Hole?). All we can say is that during this time, termed the ‘Planck era’, everything that we see in the Universe today was squeezed into a tiny dot, smaller than an atomic nucleus. The four fundamental forces of nature –gravity, electromagnetism and the strong and weak nuclear forces –were indistinguishable from one another, and the ‘dot’ was already expanding. To fully picture the Planck era, a theory of quantum gravity is needed, such as string theory (see Was Einstein Right?). As the Planck era ended, so gravity became a distinct force and physics as it is presently understood took over. However, the temperature and pressure were so extreme that matter and energy were entirely interchangeable; particles would form spontaneously from the writhing energy. Every time this happened, out too would leap particles of unusual stuff: antimatter.

>THE UNIFICATION OF NATURAL FORCES: THE FOUR FUNDAMENTAL FORCES OF NATURE WERE ONCE JOINED TOGETHER

>Antimatter The concept of antimatter first found its way into the minds of physicists in 1928 when British physicist Paul Dirac discovered an equation that correctly described the behaviour of electrons moving at high speed, but predicted that ‘mirror-image’ electrons should exist as well. These other electrons would be identical in mass but would carry positive electrical charge instead of the normal negative charge. Just four years later came the experimental proof, with the discovery of a positively charged electron (later called a positron) in a shower of particles coming from space. Dirac extended the idea to all particles of matter, and coined the umbrella term of ‘antimatter’ for the oppositely charged counterparts. One of antimatter’s properties is that, should it run into its mirror-image piece of matter, both will transform into pure energy. For example, the collision of a positron with an electron will annihilate both, giving out a pair of gamma rays.

>‘Why is there something rather than nothing?’ GOTTFRIED LEIBNIZ 17TH CENTURY PHILOSOPHER

>This leads to one of the most vexing questions for modern cosmology: why is there any matter left in the Universe? Since an antimatter counterpart is predicted to accompany every particle’s creation, it should mean that everything eventually annihilates back into energy. Yet the existence of stars and planets and galaxies shows clearly that there is a residue of matter. The solution is bizarre: mathematical calculations reveal that for every billion particles of ordinary matter created after the Big Bang, there were only 999,999,999 particles of antimatter formed. These annihilated with the corresponding matter, leaving a single orphaned particle of matter. This process repeated over and over again in the early Universe, building up enough matter particle by particle to make all the celestial objects. It means that for every particle of matter that exists today, there were once a billion other particles, but all of these were annihilated back into energy –which eventually became the microwave background radiation we see today. Cosmic inflation In the period following the Planck era when matter was being created, cosmologists believe that the Universe underwent a sudden intense expansion that they call ‘inflation’, which drove the Universe to balloon by a factor of 1050 in a time of just 10-32 seconds. The observations that led astronomers into this thinking are encapsulated in two thorny cosmological problems.

>‘The Universe is a big place, perhaps the biggest.’ KURT VONNEGUT 20TH CENTURY SCIENCE FICTION WRITER

>The first is the horizon problem. This is best explained by reference to the cosmic microwave background, which shows that the temperature on one side of the Universe is the same as on the other. The energy carried by the microwave radiation governs the temperature of space by heating any molecule or atom that gets in its way to the same temperature of about 2.7 kelvin (-270.3 degrees Celsius). The conundrum is why it should be the same everywhere, when the two far sides of the Universe cannot yet know of the other’s existence. In scientific parlance, they are said to lie outside each other’s observable horizon. Separated by at least 26 billion light years and probably much more (see How Big is the Universe?), the opposite sides of the Universe have not had time, in the Universe’s lifetime of 13.7 billion years, to exchange energy and so equalize their temperature. Yet, the entire Universe displays the same temperature, no matter where astronomers look. They do not know how this could happen, so they call it the horizon problem. The inflation theory supplies a possible answer because it states that the entire volume of our Universe came from a vanishingly small region that suddenly grew in size and spread the same temperature across space. Second is the flatness problem. Einstein’s proposition that matter curves the fabric of space means that the Universe should have an overall curvature, determined by the total amount of matter and energy it contains (see What Will Be the Fate of the Universe?). But as far as anyone can tell, the Universe is completely ‘flat’ on the largest scales, an extremely unlikely outcome of Einstein’s equations that represents a perfectly balanced cosmos. Again, inflation could provide a solution because whatever intrinsic curvature the Universe has, it has become spread over such a large scale that we can no longer perceive it. This is exactly like the way the ground beneath our feet appears flat even though we know it is part of the curved surface of the Earth. Whilst inflation helps with these problems, the query remains as to why the Universe would have inflated in this way. It may be linked to the way the strong nuclear force ‘broke away’ from the electroweak force (the still-united weak nuclear force and electromagnetic force). This separation may have produced energy and driven the inflation, but this is uncertain and the subject of much debate between physicists

>THE FLATNESS PROBLEM: INFLATION FLATTENS THE UNIVERSE, NO MATTER WHAT ITS ORIGINAL SHAPE

>Post-inflation By the time the Universe in cosmological models reaches a millionth of a second in age, there is more confidence about what is going on. This is because powerful particle accelerators, such as the Large Hadron Collider in Switzerland, can re-create the high-temperature, high-pressure conditions by smashing particles together with great energies and analysing the debris. At a microsecond, the Universe was filled with subatomic particles called quarks, together with antiquarks and gamma-ray photons. Quarks are the smallest building blocks of ordinary matter. These began to gather themselves together to form protons and neutrons, which in turn went on to form the atomic nuclei of today. As time ticked forward, the Universe continued to expand –although much less rapidly than in the inflation era –lowering the density of matter and energy, and so reducing the temperature and pressure. At around two seconds after the Big Bang, the electroweak force separated into the weak nuclear force and electromagnetism; now all four of the fundamental forces displayed their unique characteristics. This led on to the next great era of the Big Bang, which began about three minutes after creation, and ended perhaps a quarter of an hour later. It is known as the era of ‘nucleosynthesis’ and is the period George Gamow investigated mathematically during the 1940s. For these few minutes, the entire Universe was somewhat similar to the interior of a star; in this hot, dense maelstrom, some of the protons and neutrons combined to form helium and lithium nuclei. When formulating his theory, Gamow originally thought that all the elements could be forged in the aftermath of the Big Bang, but as he looked at the calculations he realized that they only worked well for hydrogen, helium and lithium; subsequent investigations confirmed that the Big Bang would not build anything heavier than lithium. The existence of heavier atoms in the Universe today became a mystery that was solved in 1954, when British physicist Fred Hoyle showed that the nuclear furnace at the centre of a star is the only place where the rest of the chemical elements can be built (see Are We Made from Stardust?). As a result, we now know that heavy elements did not appear in the Universe until nearly a billion years after the Big Bang and this has implications for the nature of the first celestial objects that could form (see What Were the First Celestial Objects?).

>‘There is a coherent plan in the Universe, though I don't know what it’s a plan for.’ FRED HOYLE 20TH CENTURY ASTRONOMER

>The Universe then entered a relatively calm phase, becoming a sea of jostling particles and photons of energy. The photons, created by the matter–antimatter annihilations, continually collided with electrons, preventing them from bonding to the atomic nuclei, and thus creating a state of matter called plasma. Plasmas exist today inside stars and in clouds of gas surrounding high-mass stars; in the early Universe the plasma stretched across all space. By the time the Universe was a year old, collisions between photons and particles had become less frequent because the continuing expansion gave the particles more space in which to move about. This allowed gravity to begin slowly pulling the particles into clumps. As time passed and the density of the Universe continued to decrease, around 380,000 years after the Big Bang one of the greatest watersheds was reached –the ‘decoupling’ of matter and energy. All of a sudden, the drop in number of electron–photon collisions meant that electrons could be captured by the atomic nuclei. Neutral atoms of ordinary matter formed, and space was cleared of its fog of particles. Some say that this was the point at which the Universe became transparent, because most photons could now travel all the way across the Universe without running into an absorbing particle. At this stage the photons made up a powerful sleet of X-rays; they have been continually redshifted by the expanding Universe ever since and now exist as the microwave background detected by Penzias and Wilson. In the future they will become weaker still, stretching out to become mere radio waves, the weakest form of electromagnetic radiation. The Universe’s baby pictures Maps of the microwave background radiation in all directions have been analysed and found to contain minuscule variations in its temperature from place to place. These ‘anisotropies’ amount to differences of no more than a hundred-thousandth of a degree, but are highly significant. They are the imprints left on the microwaves by the clumps of matter that formed in the early Universe, around its 380,000th birthday, and show us the seeds from which galaxies and the large-scale structure of the Universe emerged. To get a snapshot of the Universe at a younger age, astronomers must turn to tiny particles known as ‘neutrinos’, which appear during interactions involving the weak nuclear force. About two seconds after the Big Bang, the moment that the weak force separated from the electromagnetic force, a gigantic burst of neutrinos would have been released. This should be all around us even now, just as the microwave background fills all space. Neutrinos, however, are far more difficult to observe than microwaves. They are ghostly particles that find it easy to slip right through detectors, in fact they stream unnoticed through the entire Earth. In the time it has taken you to read this page, trillions of them will have passed through you, into the Earth and out of the other side to continue their journey across the Universe. The possibility of such flighty particles first presented itself in calculations during the 1930s, but it took almost three decades before a neutrino was captured in a purpose-built detector. The early versions of these detectors resembled giant swimming pools, but buried deep underground so that the surrounding rocks could shield them from unwanted particles. They were filled with water or some other detecting fluid and lined with sensors that would record the flash of light produced when a neutrino struck a molecule in the tank. Typically they would capture one or two neutrinos a month. Modern neutrino detectors can be found buried in the Antarctic ice, or sunk below the surface of the Mediterranean Sea. They still look for flashes of light produced by neutrino collisions, but they scan the ice itself, or seawater, for the faint flash that betrays each neutrino’s passage. It is hoped that within a decade these detectors will provide a neutrino map of the entire sky. Unfortunately for cosmologists, however, it is not the low-energy neutrinos given out two seconds after the Big Bang that will be mapped, but high-energy ones generated from the explosion of stars. Nevertheless, this will be a step in the direction towards a grabbing a picture of the Universe as it looked when it was two seconds old. Cosmologists’ ambitions do not stop there. The ultimate picture of the Big Bang may be possible if physicists can unify the forces and deduce the nature of the particles suspected to carry quantum gravity, the gravitons. By analogy with the weak nuclear force and neutrinos, the separation of gravity at the end of the Planck era would have generated a background of gravitons. There is speculation regarding a future graviton telescope, which would be able to detect these and so give a picture of the Universe just 10-43 seconds after the Big Bang. To all intents and purposes, it would be a picture of the Big Bang.

>What Were the First Celestial Objects?

>Deep fields The first attempt to see into the furthest reaches of the Universe was in 1995, when the orbiting Hubble Space Telescope pioneered the technique of ‘deep field’ astronomy. A ten-day observation was conducted that consisted of looking at a single patch of sky no larger than a tennis ball placed 100 metres away. The area chosen was one near the constellation of Ursa Major (also known as The Plough or Big Dipper) that appeared to be completely empty –nothing had ever been found there. After ten days of collecting light, however, the Hubble Telescope produced an image revealing 3000 celestial objects. The vast majority of them were small galaxies, at distances greater than 10 billion light years.

>GALAXY FORMATION

>The image became known as the ‘Hubble Deep Field’ and gave astronomers their first real look at such extremely distant realms. Previously, observations with ground-based telescopes had only detected galaxies within 7 billion light years, about halfway across the Universe. These galaxies seemed indistinguishable from present-day galaxies, suggesting to astronomers that, however galaxies formed, they did it relatively quickly, building themselves into their mature shapes within the first six billion years of the Universe’s history. Today’s galaxies are classified according to their shape (see What is the Universe?). Elliptical galaxies are elongated balls of stars; spiral galaxies are flat with a central hub of stars and arms that spiral around the centre; barred-spiral galaxies each have an elongated central hub connecting to the spiral arms. There are also irregularly shaped galaxies. The large numbers of small, distant galaxies in the Hubble Deep Field confirmed that today’s large galaxies began as much smaller collections of a few million or fewer stars. They were either irregularly shaped or elliptical, and built themselves into larger galaxies by colliding and merging with their neighbours. As they grew in size, they eventually accumulated enough mass to develop appreciable gravitational fields with which to pull in gas from intergalactic space. As the gas plummeted towards the galaxy, it fell into orbit in a disc around the central hub of stars. When the disc accumulated enough gas, star formation spontaneously began within it and surrounded the galaxy with sweeping arms of new stars. There are two subtly different patterns of spiral arms that can form. Each betrays the behaviour of the gaseous disc in that particular galaxy. First there are the ‘grand design’ spirals, consisting of two dominant spiral arms that can be traced outwards from the centre of the galaxy. Grand design spirals are caused by a rippling wave of matter that rotates around the centre of the galaxy. These ripples, or ‘density waves’, compress dust and gas as they pass, triggering star formation. By contrast, the second type of spiral galaxy, the ‘flocculent’ spiral, has a messy whorl of stars, which form without the intervention of density waves. As a cluster of bright stars forms, those stars closest to the centre of the galaxy complete their small orbits in a short time, while those further out take longer. This stretches the star-forming regions into truncated spirals; thousands of these contribute to the ‘woolly’ appearance of a flocculent spiral galaxy. Mergers and acquisitions If a spiral galaxy is left on its own, it will continually accumulate gas from its surroundings. Perhaps it will occasionally cannibalize a much smaller galaxy, but neither process will affect its overall spiral shape. However, should it veer too close to a similarly sized galaxy, the resultant collision will destroy the delicate spiral shape. As they merge, the two galaxies will lose all structure and the result will be a large fuzzy cloud of stars: an elliptical galaxy. This mighty collision will also force all remaining gas in the merged galaxies to transform into new stars, triggering a sudden explosion of star formation known as a ‘starburst’. In a few hundred million years, a multitude of brilliant star clusters will be created until all of the gas is exhausted.

>At the centre of the merging galaxies other dramatic events will be afoot. Both galaxies will contain a central supermassive black hole (see What is a Black Hole?) and these will both sink towards the centre of the merged galaxy, where they will draw each other into spiralling orbits. Their huge gravitational fields will interact, swallowing stars and throwing others into eccentric orbits. Eventually the black holes will meet and plunge together, releasing a torrent of energy that sweeps through the galaxy as a burst of radiation. The newly enlarged black hole will continue to consume clouds of gas, stars or planets that haplessly stray into its gravitational reach. For the first few million years following a merger, this can be a massive amount of material, and the merging galaxies will most likely become a quasar: a highly active, tremendously luminous galaxy. These once populated the Universe in great numbers but have dwindled to extinction, no doubt because the black holes that power them have devoured everything within their gravitational grasp. Once the quasar eventually dies down, it becomes an ordinary galaxy with a dormant central black hole. Cosmologists believe that the Universe built its current quota of galaxies in this way. But the nature of the first step of the sequence–the origin of the collections of a few million stars –remains elusive. Ultra deep fields When the Hubble Space Telescope was upgraded with a new camera, astronomers tried another deep field observation. The ‘Hubble Ultra Deep Field’, as the new shot was called, covers an area of about one-tenth that of the full Moon and revealed 10,000 small galaxies. Most of them appear as they looked around 800 million years after the Big Bang, which is staggering, but still there was no sign of the very first, individual celestial objects. They must be more distant still, and too faint to be seen by the Hubble Telescope. So astronomers have had to turn to the theorists, who use computers to model what were likely to have been the first celestial objects drawn together by gravity. There seem to be two possibilities: either they were stars, gigantic by our modern standards; or they were black holes, already busily sucking in gas that would radiate furiously as it fell into oblivion. Whichever they were, they were the objects that clustered together to become the galaxy building blocks, and prepared the Universe for the formation of other celestial objects. As stars and black holes would do these jobs in different ways, it is crucial for cosmologists to determine which it was, in order to understand the subsequent development of the Universe. Megastars or black holes? Of all the celestial objects, stars are the ones that exert the biggest influence over the Universe’s chemical composition (see What Are Stars Made From?), and none have affected it more than the earliest stars. During the ‘dark ages’ before the first luminous objects, all that existed was a diffuse sea of atoms: roughly three-quarters of it hydrogen, one quarter helium, with a seasoning of lithium. No other chemicals yet existed, and computer models suggest that this lack of variety had a tremendous effect on the first generation of stars. As gravity pulls gas together to form a star, so the gas naturally heats up as its atoms are confined. This heat resists further compression and must be radiated into space in order for the star to continue pulling itself together. Calculations show that the heavy chemical elements are highly efficient radiators, whereas the light gaseous elements find it difficult to dissipate their energy. So in star formation today, the presence of elements heavier than lithium speeds up the collapse, allowing stars to form from relatively compact pockets of gas. This results in most stars containing less mass than the Sun. Back in the dark ages, however, the forming stars did not have the help of the heavy elements in losing heat, and this meant much more gas had to build up in order for gravity to become the overwhelming force. The first stars were therefore much larger than those found in the present Universe, with masses from several hundred to a thousand times that of the Sun. One of these megastars would be big enough to engulf all the planets in the Solar System, were it placed at the Sun’s location. It was first thought that these early megastars were the chemical factories of the Universe, with their vast nuclear furnaces transforming a fraction of their hydrogen and helium into the other chemical elements and then scattering it into space, where it could be incorporated into the next generation of smaller stars. But there is a flaw in this theory. Hydrogen is transformed by nuclear fusion in one of two ways: either in a series of collisions called the ‘proton-proton chain’ or through a reaction sequence known as the ‘carbon-nitrogen-oxygen cycle’ (CNO cycle for short). The proton-proton chain is the less efficient of the two but, because in any early megastar there was no carbon to begin the CNO cycle, it would have been forced to rely on the proton-proton chain. The difficulty is that the proton-proton chain would not have been able to generate enough energy to counteract the gravity of a star of that size, and so the star should have simply collapsed –and become a black hole. A black hole made in this way would have possessed from several hundred to a thousand solar masses and could certainly have been the seed around which a galaxy began to form. In effect, these early black holes would have been quasars in the making. Yet this picture cannot be completely right either, because light collected from a handful of extremely distant quasars that were shining just 900 million years after the Big Bang shows the telltale pattern of absorption lines (see What Are Stars Made From?) that betrays the presence of iron –an element that can only be formed in the heart of a massive star. So, at least some stars must have formed before these quasars.

>‘All truths are easy to understand once they are discovered; the point is to discover them.’ GALILEO GALILEI 17TH CENTURY ASTRONOMER

>It seems likely then that a mixture of stars and black holes constituted the first celestial objects. The only way to understand exactly what was going on back in those distant times is to find a way of seeing all the way back to the dark ages. Gamma ray bursts In an attempt to detect the heat from the first stars, astronomers launched a high-altitude balloon-borne experiment in 2006. It was intended to measure the infrared radiation from the first stars, which had been redshifted into radio waves by the expansion of the Universe (see How Big is the Universe?); instead, the experimenters found that a mysterious wall of radio noise deafened their detectors. This cosmic static was six times louder than anything the astronomers were expecting and completely prevented them from observing the heat from the first stars. Cosmologists speculate that this mysterious radiation may be coming from the death throes of the earliest stars. They have good reason to suspect this because when massive stars explode, they become billions of times brighter than normal. So it seems likely that the first glimpse of something from just after the dark ages will not be an ordinary large star, but the brilliant explosion that marked its death. Indeed, the most distant object currently known is a type of celestial explosion called a ‘gamma ray burst’ (GRB). These stellar explosions appear to be more energetic than any supernova in the modern (nearby) Universe and add weight to the idea that the earlier generation of stars were more massive than those of today and so experienced more violent deaths. Each gargantuan star is calculated to have died with a titanic outburst that released as much energy as ten trillion Sun-like stars. Much of this energy was packed into a sudden burst of gamma rays that shot off across the Universe. Watching from Earth, we have no idea where the next gamma ray burst will come from. One arrives every day or two, but from a completely unpredictable direction. Not only that, but astronomers have to be really quick to spot them. Having taken billions of years to travel to us, they arrive and pass by in just a few seconds. Highly sophisticated spacecraft are lying in wait; the instant they detect a burst, they can pinpoint the explosion and, within a second, send out signals to guide other orbiting and ground-based telescopes to look at the correct location. The gamma ray burst GRB090423 is the record holder for distance, having been calculated to be 13.1 billion light years away when it exploded. Its outburst was detected on Earth on 23 April 2009 and allowed astronomers to calculate that it must have blown up just 600 million years after the Big Bang, making it an excellent candidate for being one of the Universe’s earliest stars.

>What is Dark Matter?

>WHAT IS DARK MATTER? The debate about what holds the Universe together No one has detected a single particle of dark matter, yet it has become a crucial component of modern astronomical theories. Quite literally, without dark matter, much of cosmology simply falls apart. The conviction that there must be an extra component of unseen matter in the Universe was founded in the 1930s, when Swiss astronomer Fritz Zwicky was studying the motion of galaxies around one another in the Coma cluster, 320 million light years away. He found that the galaxies were moving so fast that they should burst from the cluster: gravity was simply not strong enough to hold them in. Yet there were many such clusters throughout the Universe and none looked as if they were flying apart, so something was clearly keeping the galaxies together. Zwicky reasoned that there had to be more matter hidden somewhere in the galaxies, providing the extra gravity. He thought that this ‘missing mass’ must be hiding in titanic clouds of cold hydrogen and helium that had yet to produce any stars. But, try as he might, he could not detect these clouds. Rotating galaxies Skip forward 40 years and technology had became sophisticated enough for astronomers to measure not just the speed at which a galaxy was moving through space, but also how fast it was spinning. They could even split the galactic rotation into sections and calculate how the orbital speed of stars varied with distance from the galaxy’s centre. The astronomers were expecting to see stars on the edges of the galaxy orbiting more slowly than stars near the centre. This is the pattern displayed by the Solar System’s planets, as discovered by Johannes Kepler (see Why Do the Planets Stay in Orbit?). It is a direct consequence of the fact that gravity decreases with distance. But it turned out not to be the pattern followed in most galaxies; instead, no matter how far stars are located from the galactic centre, they all orbit with the same speed. When this was discovered in the 1970s, it presented astronomers with a problem, similar to that of Zwicky’s galaxy clusters, because the outer stars were moving too fast for the galaxy’s gravity to hold on to them. Since nowhere do we see galaxies in the process of spontaneous disintegration, the only solution seemed to be that there must be more matter concealed somewhere within or around the galaxies. GALAXY ROTATION CURVES For the stars to all move at the same speed, the amount of this matter would have to increase with distance in a way that would compensate for the expected drop in the gravitational force. This would require a sphere of matter, called the halo, to surround each galaxy; but while circumstantial evidence for this extra matter was too great to be ignored, the searches for it were finding nothing close to the proportions needed. This discrepancy threatened to plunge cosmology into crisis, until particle physics offered a way out. Enter dark matter In the effort to understand and unify the forces of nature, theoretical physicists were contemplating the need for new particles to carry energy. Theoretical predictions of particles had worked well for them in the past; for example, both antiparticles and neutrinos had been predicted to exist before they were observed. Antimatter emerged from calculations made by Paul Dirac in 1928 and was observed four years later (see How Did the Universe Form?). Neutrinos were ‘invented’ by particle physicist Wolfgang Pauli in 1930 to account for missing energy in certain reactions involving the weak nuclear force. He called his invention a ‘desperate remedy’, but explained that it was necessary either to make up a new particle or to accept that energy could vanish from the Universe. It took 26 years for the experimenters to build an experiment that detected a neutrino. In light of these successful predictions, particle physicists began talking about whole new swathes of particles in their attempt to unify the fundamental forces. These particles were so unlike normal matter that if they did exist, they would generate gravity but otherwise interact with normal matter hardly at all. Astronomers latched on to this idea immediately, realizing that these theoretical particles might be just what they needed to provide their ‘missing mass’. Indeed, when they made their calculations they found that the new matter could outweigh normal matter by anything between ten to a hundred times, making them perfect for providing the extra pull of gravity to hold each galaxy together.

>Hidden assumptions Many astronomers are coming to rely on the existence of dark matter, but others are growing sceptical. This is because dark matter has an Achilles heel in the form of a hidden assumption: its existence relies on Newton’s law of gravity being accurate and applicable to gravitational fields no matter how weak they might be. Scientists already know that Newton’s law fails in strong gravitational fields, where Einstein’s General Theory of Relativity needs to be applied instead. It is possible that the same is true at the other end of the scale, where gravity is extremely weak. In 1981, as others were beginning to embrace dark matter, Israeli physicist Mordehai Milgrom proposed a change to Newton’s law of gravity to explain rotating galaxies without the need for new particles. Remembering that gravity produces acceleration in objects (see Was Einstein Right?), Milgrom’s suggestion was that Newtonian gravity changes below a certain acceleration value, so that instead of the force dropping as an inverse square law (double the distance, quarter the acceleration), it begins to drop less sharply, as a simple inverse law (double the distance, halve the acceleration). The critical acceleration at which this happens is minuscule –no more than that produced by the gravitational field of a single sheet of paper –but it has a dramatic effect at the outer edge of galaxies. Milgrom showed that by making weak gravitational fields pull a little harder than expected, theoretical models could successfully show the stars moving with uniform speed all the way out to the extremities of a galaxy. In the vast majority of cases, this produced better agreement with observations than the dark matter models. He called his idea ‘Modified Newtonian Dynamics’ (MOND) and, although it currently remains a minority view, if the dark matter detection experiments fail to produce any results, and neutralinos are not produced at the Large Hadron Collider, then perhaps more and more astronomers will contemplate what once seemed impossible: that Newton’s theory of gravity needs an update. The drawback that some see with MOND is that it has no theoretical underpinning. This makes many astronomers reluctant to take it seriously, but there are historical precedents for this kind of ‘discovery first, understanding later’. Kepler’s laws of planetary motion had no theoretical underpinning when he first proposed them early in the 17th century. He simply scrutinised the data and found an equation that reproduced them. Only in 1687 did Newton’s Theory of Universal Gravitation provide an understanding of why Kepler’s laws worked. Throughout the history of astronomy, laws have been deduced from observations before theories could explain them. Yet MOND is not flawless; the hypothesis struggles to reproduce the motion of clusters of galaxies, requiring more matter than can be seen to make things work. So are we back to needing exotic dark matter? Possibly not, as astronomers think that there may be a substantial amount of normal matter hiding in galaxy clusters, in the form of warm gas. If they can detect this, using telescopes sensitive to the ultraviolet light it is believed to emit, it would solve MOND’s difficulties with galaxy clusters.

>What is Dark Energy?

>Dark energy equals missing mass One of the earliest conclusions that Einstein drew from his Special Theory of Relativity was that mass and energy are interchangeable. He investigated, mathematically, the effects of placing energy into space and found that it causes a curvature of the space–time continuum, just as mass does (see Was Einstein Right?). He called the amount of energy naturally occurring in space–time the ‘cosmological constant’. It exists rather like the energy in a glass of water at room temperature: this energy may not be immediately obvious, but it must be removed before the water can freeze. Working before Edwin Hubble’s discovery of the expanding Universe, Einstein calculated the amount of energy –the value of his cosmological constant –necessary in space–time to resist all the gravity generated by the various celestial objects and prevent the Universe from collapsing. When Einstein learned of Hubble’s discovery of expanding space, he considered that his cosmological constant was superfluous and famously called it his biggest blunder. The realization of the accelerating Universe, however, has made astronomers think there may be unseen energy in space–time after all. If the ‘vacuum’ of space contains enough energy, it will overcome the force of gravity between the celestial objects and drive the Universe to expand at an ever-accelerating rate. Physicists tend to refer to this as ‘vacuum energy’, whereas astronomers have taken to calling it dark energy, to underline its mysterious nature. Whilst it may sound similar to dark matter, and indeed a few researchers are trying to find links between the two, most believe they are nothing to do with each other because they work on different scales. Dark matter was introduced to solve the movement of galaxies; dark energy was invoked to solve the accelerated expansion of the whole Universe.

>‘The Universe is made mostly of dark matter and dark energy, and we don't know what either of them is.’ SAUL PERLMUTTER CONTEMPORARY COSMOLOGIST

>To account for the observed acceleration, the density of dark energy needs to be only very low, which is probably why we do not see its effects at small cosmological scales, such as within the Solar System or even our whole Galaxy, the Milky Way. Only when a vast swathe of space lies between a distant object and us does the cumulative effect of dark energy become evident. When the amount of it is totalled up across the entire Universe, it becomes overwhelming, accounting for around three-quarters of the mass and energy in the cosmos, and rendering space almost perfectly flat.

>Are We Made from Stardust?

>The CHNOPS recipe Each step, from inorganic chemicals born in stars to living cells, involves crossing boundaries between the three traditional sciences of physics, chemistry and biology. Across each boundary, matter begins to behave differently and manifest unanticipated properties. For example, put enough subatomic particles together and they organize themselves into atoms based on the laws of physics. As soon as these atoms start to interact with one another, physics hands over to chemistry because the panoply of chemical interactions is difficult, if not impossible, to predict from the laws of physics. When a sufficiently complicated network of chemicals comes together, life spontaneously emerges and chemistry can no longer predict the rich variety of behaviours. To tackle the problem of how life began on Earth, we need to go back to the final stages of the planet’s formation, when it was pummelled with asteroids and comets (see How Did the Earth Form?). This ‘late bombardment’ began 4.6 billion years ago and lasted approximately 700 million years. It brought water and other volatile materials to the planets. (In this context, volatile describes chemicals that vaporize at relatively low temperatures, such as water, carbon dioxide, methane and ammonia.) All of the elements contained in the molecules of these volatile substances are from the so-called ‘CHNOPS’ range of elements, upon which life on Earth is based: carbon, hydrogen, nitrogen, oxygen, phosphorus and sulphur. Of these, oxygen happens to be the most abundant element on Earth, making up nearly half the mass of our planet, with most of it bound into rocks rather than found in the atmosphere. Similarly phosphorus and sulphur are found in the rocks from which Earth is made. In other words, the late bombardment brought many of the vital ingredients for the subsequent development of life on Earth.

>The late bombardment created hellish conditions on the Earth. It melted the crust of the planet, threw molten rock into the atmosphere, and evaporated the fledgling oceans. Nothing could seem more opposed to the development of fragile biological molecules, yet intriguingly the evidence suggests that life began soon after the late bombardment began to tail off. There was no sudden end to the bombardment, but a gradual reduction of collisions over time, and by 3.9 billion years ago the impacts had dwindled so much that the late bombardment was effectively over. Scientists find the first evidence for life in rocks dating from just 100 million years after the bombardment stopped. The indication is an enrichment of the lightest stable carbon isotope, carbon-12, which life uses in preference to its heavier cousins because the lighter variety can pass more easily through cell membranes. Wherever living creatures have died, they tend to leave behind an enrichment of carbon-12. So, finding the isotope in those ancient rocks has been taken as a sign that microbes were present on Earth relatively soon after the fury of the late bombardment had ceased. The first fossils are found in Earth rocks dating back 3.5 billion years ago, in western Australia; these are microfossils, which are the preserved remains of ancient bacteria, the pre-historic equivalent of pond scum.

>Life in a bottle In the 1950s, chemists Harold Urey and Stanley Miller conducted an experiment that tried to simulate the early Earth. They based their work on the hypothesis of Russian biochemist Alexander Ivanovich Oparin, who in the 1920s, had been the first to suggest that that there is no magical difference between living and non-living matter, that the characteristics of life simply emerge from a sufficiently complex arrangement of matter. He also suggested, spurred on by the discovery of methane in the atmosphere of Jupiter, that methane and its volatile cousins, water and ammonia, were the chemical ingredients from which life formed. Urey and Miller set out to test Oparin’s ideas. They filled a flask with the chemicals they believed existed in the early Earth’s atmosphere –chiefly methane and ammonia –and then applied electrical sparks to simulate lightning. As the electricity caused the gases to react together, longer molecules were formed, which dropped into a small pool of water at the bottom of the flask and formed a tarry substance. Upon analysis, this thick gunk was found to contain amino acids, which are the building blocks of proteins. It seemed, miraculously, as if the first step in the process towards life might have been found. However, as other scientists built on this work, they came to the conclusion that Earth’s early atmosphere was more likely to have been composed principally of carbon dioxide. This was bad news because when the Miller–Urey experiment was re-run with a carbon dioxide atmosphere it was nowhere near as successful at producing amino acids. But just when scientists faced a dead end, a new clue landed in their laps –almost literally. The Murchison meteorite It was 28 September 1969, late morning in the quiet town of Murchison in Victoria, Australia. A burning fireball split the sky, broke into three and disappeared from view, leaving a smoke cloud hanging. Many fragments of the meteorite were soon found, totalling more than 100 kilograms, and were identified by visiting academics as a rare form of space rock known as a ‘carbonaceous chondrite’. Samples were rushed to NASA for analysis, where scientists were amazed to find more than 90 different amino acids within the meteorite material. This clearly indicated that amino acids were assembled in space and brought to Earth during the late bombardment. However, it seems that the Earth exploited only a small fraction of these amino acids to create proteins –just 20 amino acids go together in different combinations to make up the millions of different proteins used by life on Earth. So the puzzle is how the multitude of 90 or more amino acids led to a functioning life form using just 20. It is believed that all life today evolved from a single common ancestor, the first organism to form and presumably a very simple living thing. To understand how it came about, a good place to start investigating is amongst the smallest living things on Earth today: microbes. The bacterium E. coli is the ‘laboratory standard’; it is rod-shaped and just three millionths of a metre in length. It contains 4377 genes, as compared with the human genome which contains about 40,000. The genes hold the blueprints for the proteins needed to make the life form function. In the case of humans, genes control everything from musculature to eye colour. The common thread between microbes, humans and all other forms of life on Earth, is that all genes are contained in molecules of DNA, deoxyribonucleic acid. DNA is a long molecule, the backbone of which is a chain of carbon atoms. From this carbon spine hang the genes, each composed of a sequence of chemicals. There are two complementary strands that wrap around each other to form the famous double helix, locking the genes inside. But at specific times the strands can unwrap and make copies of themselves; this ability to replicate lies at the heart of all living things. Yet the replication is not a perfect process; errors creep into the genes when they are being copied and whilst this might seem like a bad thing, it is actually what drives evolution. Although the errors, known as ‘mutations’, mostly make the proteins behave less successfully, occasionally they improve their function and the life form flourishes, increasing the chances that the favourable mutation will be passed on. By charting mutations and by showing when they diverged from one another, biologists can build up a tree of life that shows how organisms are related. Human beings and other mammals lie near the top of the tree, above simpler animals such as reptiles and insects. Towards the base of the tree are the microbes such as E. coli, and sitting closer to the bottom of the tree than anything else is a group of microbes called the hyperthermophiles.

>All animal life on Earth requires oxygen, yet various extremophiles can live off hydrogen, iron or many other chemicals that would spell certain death for most living things. Nowhere on Earth contains more readily available chemicals than the volcanic vents on the sea floor, where they are dissolved in the hot water gushing up into the ocean. This makes each volcanic vent a haven for extremophiles. Sea-floor volcanic vents are the underwater equivalent of geysers, such as Old Faithful in Yellowstone National Park. Often known as ‘black smokers’ because the dissolved minerals condense into black clouds as soon as they hit the frigid water surrounding them, they were first found near the Galapagos Islands in 1977. Most surprising of all, the analysis to place their organisms on the tree of life revealed the hyperthermophiles to be the most ancient forms of life on the planet. This could mean that black smokers are the actual sites for the origin of life; they certainly offer some advantages because the hyperthermophiles are the only known colonies on Earth that do not rely on sunlight for energy. If the Sun went out tomorrow, the communities around the black smokers would continue to thrive. Their energy comes from volcanic activity, driven by radioactivity within the Earth, and so they live quite independently of the Sun. This makes them immune to almost anything going on at the surface: ice ages or other climate catastrophes, even the last years of the late bombardment could have taken place without threatening them. But there is one inconvenient fact that casts doubt on the black smokers as the site of life’s origin. The ‘tree of life’ analysis shows that the hyperthermophiles are indeed ancient but cannot be the common ancestor of all life on Earth. The microbes that evolved into us split away before the hyperthermophiles developed; this is indicated by the fact that hyperthermophiles contain genes that we do not. So, life may have developed elsewhere and some form of it then migrated to the black smokers. We have not yet been able to discover the earliest life forms, either because they are extinct and have left no traces, or because we have yet to look in the right place. This has led some scientists to think that life’s original form may be even less complicated than a microbe.

>Can We Travel Through Time and Space?

>Imposing a speed limit The universal speed limit came about as a consequence of the principle that the speed of light is a constant. By this Einstein meant that regardless of the speed of the measurer, the speed of light always appeared to be the same. This is contrary to what was expected, because in our normal experience velocities ‘add’ together; two cars travelling at 50 kilometres perhour towards each other pass with a combined, or relative, velocity of 100 kilometres per hour. This is not true for light. If you measure the speed of light the value will always be the same, no matter whether you approach the light beam head-on, from the side, or are running away from it. This had been experimentally proven by the Michelson–Morley experiment in 1887.

> ‘Space isn’t remote at all. It’s only an hour’s drive away if your car could go straight upwards.’ FRED HOYLE 20TH CENTURY ASTRONOMER

>American physicists Albert Michelson and Edward Morley originally designed their experiment to provide incontrovertible evidence for the ether. At the time it was widely believed that light needed a medium to travel through, rather like sound travelling through air, and the postulated medium in which the Earth was immersed was the ether. Michelson and Morley reasoned that as Earth travelled around its orbit at 30 kilometres per second (19 miles per second) it should experience an ethereal wind. The wind would sweep across Earth in different directions and at different speeds, depending on the time of year and the direction in which the Earth was moving. To measure the effects of this wind, Michelson and Morley split a beam of light into two using a semi-silvered mirror so that half of the light passed straight through whilst the other half was reflected. They then sent these identical beams down two paths at right angles to one another, and eventually combined the beams on their return journey. Travelling in different directions relative to the Earth’s motion, the two light beams should have been affected differently by the etheral wind: one beam would meet it head-on, the other would feel it broadside. However, the light beams were not affected in any discernible way. No matter when the experiment was conducted, the same null result was obtained: it was as if the ether did not exist at all.

>Eventually this is what the physics community decided: there is no ether. Light does not need a physical medium, and its perceived speed does not depend on the speed of the observer. Accepting this result, Albert Einstein set about investigating what consequences this would have, and it led him to the Special Theory of Relativity. ‘Relativity’ here refers to the fact that speeds can only ever be measured relative to something else. There is no universal standard, no absolute framework of space that speeds are measured against; one object must always be compared to another. ‘Special’ refers to the fact that this is not a result that can be easily applied to all forms of motion; at first, Einstein investigated only non-accelerated motion, in other words the simple case where objects were not changing their speed or direction in any way. He subsequently extended his investigation to accelerated forms of motion in his General Theory of Relativity (see Was Einstein Right?). While working on special relativity, Einstein found that once an object’s velocity exceeds ten percent the speed of light, previously unimaginable effects manifest themselves. As bizarre as it may seem, the object becomes more massive the faster it travels. This increase in mass means an increase in inertia: more energy is required to make the object go faster. So, as the object accelerates further, becoming more massive all the time, greater and greater amounts of energy would be needed; in fact to accelerate an object to the speed of light would require an infinite amount of energy. It follows that it is impossible to attain the speed of light: it is a fundamental speed limit. On the face of it, we seem restricted forever to meander around the confines of the Solar System like ants on Earth’s surface, restricted to a minuscule area because we simply do not live long enough to cross the vast distances between the stars. It is a frustrating thought –but there is a get-out clause.

>Time travel If all the talk of warp drives and suchlike sounds fanciful, putting those ideas into practice will be a walk in the park compared to building a time machine. First, we need to understand what we mean by time. Time is an immensely difficult concept to define; unlike electrical charge or mass, it is not something that can be measured. This may not be obvious, as we are so used to monitoring the passage of time in our lives. But clocks do exactly this –mark the passage of time –by using some phenomenon in which time is an integral factor, such as the oscillation of a quartz crystal or the decay of a radioactive isotope; nothing actually measures time itself. Nor is time like shape, taste or colour, because it cannot be perceived by our traditional senses; yet we are constantly aware of its passage by the changing nature of events around us, or just by our own changing thought patterns. We are travelling through time, one-way, into the future. The passage of time can be slowed down, however: special relativity tells us how to do this. As well as the increase in mass experienced when something travels close to the speed of light, another bizarre special relativistic correction involves time. Known as ‘time dilation’, this states that the faster an object moves, the slower time passes for it. This seems to offer something of a solution for interstellar travel because, if it were possible to accelerate a spacecraft to relativistic velocities, time would slow down inside it, allowing humans to reach the stars within their lifetimes. The downside is that outside the spacecraft time would continue to run at its normal rate and many years would pass, perhaps centuries. Imagine that one of a pair of identical twins becomes an astronaut and leaves Earth on a spaceship capable of travelling at a significant fraction of the speed of light. Upon his return, he has barely aged but his twin will now be an old man because he has experienced the passage of time differently. General relativity, too, offers some ideas here. It describes how time is slowed down in the presence of a gravitational field. Where the field is weaker, for example at the altitude of the International Space Station, time would pass quicker: a clock on the space station would gain about 1 second every 10,000 years compared to an identical clock on the Earth’s surface. This may sound small but with atomic clocks accurate to better than one part in a trillion, the time-dilating effects of general relativity can be easily be verified.

>If time travel is possible then there are a number of paradoxes that immediately spring to mind, such as the scenario of going back in time and murdering a grandparent. A number of scholars have suggested that something would always happen to prevent logical paradoxes. But to most physicists this has the tinge of a supernatural hand of fate and they prefer to believe that either the Universe might split into parallel realities (see Are There Alternative Universes?) or that time travel is simply impossible.

>Fermi discounted the weird and wacky UFO sightings as evidence of anything, and his simple question became known as the ‘Fermi Paradox’. He used it to argue that practical interstellar space travel was impossible; otherwise the evidence of extraterrestrial visitation should be all around us. The same argument can be advanced about time travel. If it is possible, someone somewhere at some time in the future will invent a time machine; at which point people will begin travelling into the past, and should be among us today. So, if the laws of physics really permit time travel, where are the visitors from the future?

>Can the Laws of Physics Change?

>The constants of nature There are many so-called constants in nature. They are the values that cannot be derived from theory, and so can only be determined by measurement. They are used in the laws of physics as conversion factors to create exact mathematical relationships between quantities. In the case of gravity, mass in kilograms and distance in metres are equated to a force in newtons by Newton’s ‘gravitational constant’. This is often referred to as ‘Big G’ because it is denoted in the equation by a capital G. Some of the constants are self explanatory, such as the speed of light. Others seem more abstruse, such as the Planck constant, which governs the way nature breaks energy up into small ‘packets’. Despite calling these quantities constants, there has been a creeping suspicion over the last 15 years or so that some of them might be changing slowly with time –particularly the speed of light. In 1993, physicist John Moffat published his solution to the cosmological horizon problem. This is the tricky observation that the temperature of the cosmic microwave background is virtually the same regardless of which direction we look (see How Did the Universe Form?), so completely disconnected regions of the Universe have somehow reached the same temperature. Traditional physics can only explain this if the Universe was driven into a sudden period of exponential expansion, known as ‘inflation’. However, inflation does not have any firm foundations in physics, meaning that what actually drove this supposed expansion remains a mystery. This shortcoming led some to look for alternative theories of temperature equalization. Moffat pointed out that if the speed of light had been higher in the past, photons of light could have travelled much further and so could have equalized the temperature across a much wider expanse of space without the need to invoke inflation. Other physicists used the same idea to perform a new analysis of the cosmological flatness problem (see How Did the Universe Form?), and showed that they could also account for this without inflation, providing that the speed of light was extremely high during the first moments of the Universe’s life and then fell quickly to near its modern value. Astronomers cannot directly test for such a circumstance since they cannot see the fleeting period of time immediately after the Big Bang. But they can study distant quasars –early galaxies powered by matter falling into black holes (see What is a Black Hole?) –in the hopes of catching the last vestiges of any change in the speed of light. In order to detect such a change they look at something called the ‘fine-structure constant’, which defines the strength of the electromagnetic force in relation to the other forces of nature and determines the pattern of spectral lines from light sources (see What Are Stars Made of?). Its value depends on the speed of light and, importantly, it is what scientists call ‘dimensionless’. Dimensionless constants Physicists have to be careful when drawing conclusions from the measurement of constants that have units attached to them. For example, the speed of light is measured in metres per second, or any other units of length and time chosen. If a variation is measured, the researcher cannot be sure whether it is the speed of light that has truly varied; it could just as well be the rate at which the clock has ticked, or the length of the ruler, that has changed. To avoid this confusion, physicists tend to concentrate on examining dimensionless constants when searching for natural variability. If you measure the ratio of, say, the proton’s mass to the electron’s mass, then the units –kilograms –will cancel out and the resulting constant you get will simply be a number. If something weird happens to the way you define a kilogram, that will be cancelled out in the ratio and not affect your conclusion. So, if the value of the ratio changes by even the smallest amount, you can be certain that at least one of those masses is actually changing in some way. The fine-structure constant is just such a dimensionless constant. It is obtained by combining the speed of light with the Planck constant of energy and the charge on an electron. It affects the outer structure of each atom, which controls the way the atomic electrons react with passing light beams. If the speed of light were to change with the passage of time, the fine-structure constant would also change, and the characteristic spectral lines of all the atoms would change as well. This is exactly what one group of astronomers believe they have seen. In 1999, John Webb of the University of New South Wales used the world’s largest optical telescope to observe 128 quasars out to distances of around 10 billion light years. Webb’s team collected the quasar light, split it into spectra, and looked for the fingerprints of intervening atoms. Their analysis showed that the spectral lines changed in a way that was consistent with the fine-structure constant having increased slightly during the course of cosmic history, by around 1 part in 100,000 during those 10 billion years. Numerous groups are now working to verify, or disprove, the variation of the fine-structure constant. If they confirm it, then scientists will have to decide which of the constants that define it is actually varying. Is it the speed of light, the charge on an electron or the Planck constant of energy? Most people suspect it is the speed of light, because of the way a change could help solve the horizon problem of uniform temperature across the Universe. Nevertheless, the discovery of a constant changing has enormous consequences for our understanding of the Universe. It points to physics beyond Einstein, perhaps even to the elusive ‘theory of everything’. Most physicists believe that the best candidate for a theory of everything is string theory (see Was Einstein Right?). This complex mathematical theory replaces particles with wiggling strings, but the wiggling take place in higher dimensions than the three we are directly familiar with. We see the strings as particles because, rather like icebergs, there is a lot going on ‘beneath the surface’. According to string theory, only if all the higher dimensions are taken into account will the value of physical constants remain truly constant. Hence, string theory allows for the constants of nature to appear to change in the dimensions that we perceive. If we could measure a change, it would allow us to use string theory to prove the existence of higher dimensions and to see how they are behaving.

>‘There are grounds for cautious optimism that we may now be near the end of the search for the ultimate laws of nature.’ STEPHEN HAWKING CONTEMPORARY PHYSICIST 

>Big G The strength of gravity has been another target for physicists searching for variations in the constants of nature. The difficulty is that Big G, which encapsulates the strength of gravity, is one of the most elusive constants in nature because despite sculpting the Universe on its largest scales, gravity is the weakest of the forces. This means that Big G is difficult to measure accurately. It was over a century after Newton’s derivation of universal gravity that the first successful measurement of Big G was made, and another century before the realization sank in that this is what had been achieved. Working in 1797, Henry Cavendish managed to measure the force of gravity generated between two lead balls, one 12 inches in diameter and the other much smaller. He did this using a piece of equipment called a torsion balance, which transformed the small gravitational force between the two weights into a twisting of the apparatus, which could be seen and measured. Cavendish then weighed the smaller lead ball, which gave him the force of the Earth’s gravitational field, and compared the two forces to obtain the density of the Earth. This was a quantity much desired by astronomers of the time, because they could use it to calculate the density of the Solar System’s other objects. It was only in the late 19th century that scientists came to view Newton’s gravitational constant as something fundamental to science. And then they went back to Cavendish’s data to calculate a value for Big G. Since then Big G has been measured to greater and greater accuracy, even though there have been some hiccups along the way. In 1987, scientists thought Big G was known to an accuracy of 0.013 percent. Improved experiments in 1998 forced this to be re-assessed to a lesser accuracy of just 0.15 percent. Even today, the value of Big G is extraordinarily imprecise when compared with the force of electromagnetism, which is known to 2.5 million times greater accuracy than is gravity. It is this lack of precision that has led to most of the speculation about whether the constant might be changing in value slowly over time, in effect changing the strength of gravity. Such a variation would gradually change the orbits of stars and planets, affect the sizes of celestial objects, and even determine how brightly stars shine. Most recently, lunar laser ranging experiments (see Was Einstein Right?) have shown that the value of Big G cannot have changed by more than one part in a million per year, otherwise their sensitive measurements would have picked up this variation in their 40-year observation of the Moon’s orbit. This does not mean that Big G has remained constant; it simply indicates that any variation has been smaller than one part in a million. So astronomers continue to collect lunar laser ranging data in an attempt to extract ever-finer measurements and search for long-term effects. At the same time, other physicists are searching for temporary changes in the strength of gravity brought on by the movement of Earth around its orbit. This could give us a clue to the physics beyond Einstein.

>Are There Alternative Universes?

>ARE THERE ALTERNATIVE UNIVERSES? Schrödinger’s cat and the implications for us all There is an exact 50-50 chance that the cat is alive, or is dead. Until we look at the cat, we do not know. Can we know? Can the state of the cat be described before we look at it, other than by a quantum superposition of ‘alive’ and ‘dead’? It must rank as one of the most famous cases of animal abuse in history. Thankfully, though, it is an entirely fictional thought experiment. Called ‘Schrödinger’s cat’, the idea is that a cat is put in a sealed box with a device containing a radioactive atom that has a 50-50 chance of decaying within an hour. If the atom decays, a flask of poison is broken and the cat dies. The Austrian physicist Erwin Schrödinger constructed this macabre ‘experiment’ in 1935 to make a point: he thought that although the newly developed quantum theory could predict the behaviour of particles, it could not be a true description of reality because it led to bizarre consequences. Specifically, quantum theory allowed particles to possess contradictory properties until they were measured. To highlight what he saw as an insoluble problem he asked how the contents of the box could be described at the end of the hour before somebody peeped inside. The half-alive, half-dead cat The condition of the cat obviously depends upon whether the radioactive atom has decayed or not, and this decay depends on probability. The physics of probable states is the basis of much of quantum theory (see What is a Black Hole?). In Newtonian physics, everyday objects interacted in ways that were rigidly repeatable and as predictable as clockwork. When someone threw a ball in the air, there was no doubt that it would fall back to the ground; the only question was how long it would remain in the air and Newton showed that this could be precisely calculated. On the scale of individual atoms, however, probability enters the mix and physicists have to use quantum theory. This shows us that events are not rigidly repeatable in the subatomic realm, so probabilities need to be ascribed to outcomes. Hence, in radioactive decay, there is a calculable chance that an atomic nucleus will decay within a given time, but there is not a guarantee. What makes the nucleus ‘decide’ to decay at a particular instant is completely unknown to us; we simply have to accept that probability is hard-wired into the Universe. Anything that relies on the inherent uncertainty of subatomic processes is said to be a quantum system. Returning to Schrödinger’s cat, quantum theory can provide an equation that perfectly describes the radioactive atom in a state that represents both decayed and yet-to-decay possibilities. But what does this mean for the cat? Must the cat be thought of as simultaneously alive and dead until the box is opened? Schrödinger considered this ludicrous.

>‘Anyone not shocked by quantum mechanics has not yet understood it.’ NIELS BOHR 20TH CENTURY PHYSICIST

>Once the box is opened, the mystery is solved. The cat will be either alive or dead, depending on whether the radioactive atom has decayed or not. But this leaves us with another dilemma: what happens to the ‘unused’ state of the atom, the alternative possibility that was not observed? Did it simply cease to exist when the box was opened? Danish physicist Niels Bohr did indeed think the alternative state just vanished. Puzzled by how to interpret the mathematics of quantum theory, he and colleague Werner Heisenberg had decided in 1927 that the act of observation forces the quantum system to ‘make up its mind’, and become one thing or the other. Prior to the observation, the quantum system was in a mixed state, a ‘superposition’ as physicists call it, of all possible outcomes. This is known today as the ‘Copenhagen interpretation’. At its heart, the Copenhagen interpretation says something exceptionally profound –that measurement creates reality. The common-sense problem with this view is, as Schrödinger pointed out, that we have a zombie cat for an hour, half-alive half-dead, until the box is opened and somehow the act of observation makes it either dead or alive. Instinctively this sounds wrong –why should the act of observation be essential in creating reality? Surely we have to believe that the Universe is ‘solid’, even when we turn our backs on it. For the first time, physicists were faced with a fundamental problem that verged on philosophy: does quantum theory describe reality or is it just a mathematical trick that gives the right answer?

>If the Universe curved completely back on itself in the fourth dimension, then a powerful telescope on Earth could, in principle, see the Milky Way Galaxy apparently very far off in space. But of course the light would have been travelling for billions of years and so the Milky Way would appear much younger than it is today. It would be like seeing all the way round the curve of the Earth and observing the back of your head far off in the distance –but you yourself would be a baby. Looking for repeating patterns in the cosmic microwave background radiation is the somewhat more practical equivalent of searching the distant galaxies for a young Milky Way. To date, no such repeats have been found and this is taken as evidence that the Universe extends further than we can see –beyond 13.7 billion light years. The theory of inflation, that the Universe underwent a sudden period of exponential expansion just after the Big Bang (see How Did the Universe Form?), if proven, would mean that the Universe must be vastly bigger than this; in fact most cosmologists believe that inflation leads to an infinite Universe. Even if inflation is proved wrong, an infinite Universe is still a possibility. If the Universe is infinite then every outcome –no matter how unlikely –is played out somewhere. Somewhere in the Universe there is an alternative Earth where the alternative ‘you’ wrote this book and the alternative ‘me’ is reading it. Think of any possibility that does not contravene the laws of physics and it will have happened. Tegmark calls these level I parallel universes. They possess the same laws of physics but started from different initial conditions, hence they are not quite the same. As time goes by, light will arrive from further and further away and these remote regions will come into view, gradually revealing these alternative universes to us. Chaotic possibilities A slightly different version of inflation, known as ‘chaotic inflation’, makes it possible that new universes sprouted away from our own because of the way that quantum theory works. If this happened, it would have set into motion a chain reaction that continues somewhere in the multiverse today –in short, a never-ending sequence of other universes being born. These comprise Tegmark’s level II parallel universes. Unlike the level I alternative universes, they do not just lie a long way away, but inhabit entirely different dimensions of space. In one of these level II parallel universes, the way the forces of nature evolved may be different from the way this happened in our Universe, and so the strength of these forces could be different. This would be reflected in the constants of nature taking on different values from the ones in our Universe. Perhaps gravity is a bit stronger and so stars are pulled together more quickly, burn more brightly and hence live shorter lives. Or the strong nuclear force is a little weaker, making more atoms radioactive; this would generate more heat inside planets, creating more volcanic activity. Some of the parallel universes may be ‘flat’, with only two extended dimensions, whereas some may have four spatial directions, or six, or none. There is no fundamental difference between the other parallel universes and our own; all have the same laws of physics. We believe that a universe transforms itself, from the high-energy state just after the Big Bang to the low-energy state of today, in an essentially random process. The strengths of the forces, the number of dimensions, even the variety of particles, are all fixed by this unpredictable process. Think of it as a ball whizzing around the top of a roulette wheel. Each ball that the croupier spins begins in a high-energy state indistinguishable from any other spin of the wheel. When it loses energy and falls down into the wheel it eventually ends up in one of the numbered slots, each one an equally valid outcome. In the case of a roulette wheel there are 37 or 38 different pockets, but for a collapsing universe there are an infinite number of possibilities. So some level II universes will be similar, even identical to ours, whilst others will be vastly different. Weirder parallel universes In his next category, the level III parallel universes, Tegmark turned his attention to Schrödinger’s cat and Everett’s original suggestion of ‘alternative realities’ in which every possibility is played out. Tegmark found a subtle but important difference between these parallel universes and the previous two types. It is to do with how they are created. According to Everett’s many-worlds interpretation of quantum theory, the Universe splits when a quantum decision is revealed –such as when the box in Schrödinger’s cat experiment is opened. This whittling of possibilities into a certainty is known as ‘decoherence’ and, until the mid-1990s, physicists did not know how it happened. In refuting Everett’s ideas, Bohr had talked cryptically about the act of observation being needed to force the quantum system to make its decision, but he failed to say what defined an observer. Was the cat in Schrödinger’s thought experiment an observer of its own condition, or was human self-awareness required? Could particles themselves be ‘observers’ by dint of their physical interactions? An experiment conducted by Serge Haroche and colleagues in 1996 using rubidium atoms and microwaves provided the answer by showing that decoherence occurs as atoms interact with their surroundings. No intelligent observers were needed, just the random interaction of other particles. The conclusion therefore is that particles are indeed Bohr’s ‘observers’ and that the act of observation is equivalent to a physical interaction between particles. This solves the worst aspect of Schrödinger’s cat experiment, namely the period in which the animal is half-dead, half-alive. This scenario never happens because the interaction of the particles inside the box –the atoms in the cat, the radioactive particle, the molecules in the air and in the poison –will mean that if the bottle breaks and releases the poison, the cat will be killed instantaneously as our common sense would suggest. The many-worlds interpretation can be thought of as offering a ‘life after death’ for the timelines that are rejected in our Universe. As one possibility comes to an end for us, so a new universe springs into existence to play it out. But there is a remarkable coincidence here: the possibilities played out in the level III parallel universes will be identical to those played out in level I examples. It is just that level I universes are separated from us by vast tracts of space. Level III universes supposedly pop into existence, presumably in some other dimension, as our reality unfolds. As yet, no one can reconcile these two similar yet different concepts. It might be thought on reading this that every time we make a conscious decision, other universes are conjured up, but such an extrapolation would be wrong. This behaviour is restricted to quantum processes. The only way our conscious decisions could create alternative realities is if somewhere deep inside the brain a decision is based upon a single quantum particle that spontaneously changes its state. This tiny happening would then need to be amplified in our consciousness to become a decision. Intuitively, this feels wrong, since decisions seem like a much higher processing of information. Each of us makes decisions by weighing evidence and past experiences and ‘computing’ what we hope will be the best course of action, not by a random alteration of a particle from one quantum state to another. ‘If we look at the way the Universe behaves, quantum mechanics gives us fundamental, unavoidable indeterminacy, so that alternative histories of the Universe can be assigned probability.’ MURRAY GELL-MANN CONTEMPORARY PHYSICIST Flipping a coin to provide an outcome is not a quantum process either. A coin flip is governed by factors that are hard to predict, but not because of quantum uncertainty. It takes place on a scale much larger than the scales at which quantum effects apply. So, sadly, deciding whether or not to finish this chapter is not going to cause an alternative universe to spring into reality –you may as well read on.

>Proof of parallel universes A strategy for how to search for level III and level IV universes currently eludes physicists, but there is a way to verify that level I and level II universes exist. Astronomers are currently searching for proof that inflation happened. If they find it, they will also have proof that parallel universes exist, because any form of inflation is thought to create level I parallel universes, which would be very far away in space and have the same values for their physical constants. If the variation known as chaotic inflation is confirmed, there will also be level II universes,‘budding’ off from our Universe and having different values of the physical constants. Inflation leaves its mark on the cosmic microwave background radiation in the form of fluctuations in the density of the cosmic matter and in the orientation of the cosmic microwave background rays. Many observations of the microwave background have been broadly consistent with inflation, although some discrepancies have been uncovered. The recently launched European spacecraft Planck will investigate further by taking highly accurate pictures of the microwave background. The orientation of the microwave radiation is known as ‘polarization’ and should have been imprinted on the microwaves during inflation by gravitational waves coursing through the Universe in the split second after the Big Bang. The gravitational waves moved through the fabric of space–time like ripples on a pond; as they passed, they would have alternately squeezed and then stretched matter. The different versions of inflation theory impose different patterns on the microwave background. If inflation of one type or another is proven to be true, then physicists will have to accept the fact that parallel universes do exist. And all of us will have to come to terms with the idea that there are many different versions of each of us out there somewhere.

>What Will Be the Fate of the Universe?

>Slow death The alternative scenario is that of the open Universe, where the density of matter is less than the critical value. The shape of space in such a Universe is more complex; the best two-dimensional analogy is rather like the upper surface of a saddle, with one dimension curving upwards and the other curving downwards. Unlike a saddle, however, it extends infinitely in all directions. The open Universe will expand forever, but this does not mean that it will remain more or less the same forever. Stars will continue to live and die much as they have done over the last 13 billion years, but space itself will change around them. As space continues to expand, galaxy clusters will be driven ever farther away from each other. Most of the galaxies that draw our attention today will eventually be lost from sight; only the stars in the Milky Way and in the 30 or so nearest galaxies in our own cluster will remain visible to us. For all the rest, their light will be redshifted away from visible light, into the infrared and then into the weak radio region of the spectrum. Perhaps the only clue future beings will have of the existence of other galaxies will be a faint radio hiss coming at them from all around, similar to the cosmic microwave background of today. The already weak background radiation itself will have been redshifted too, rendering it unobservably feeble. Future civilizations will thus have no evidence that the Big Bang ever took place (see How Did the Universe Form?). Within each galaxy cluster, where gravity can resist the cosmic expansion of space, the individual galaxies will all eventually coalesce. As the galaxies collide, some stars will be flung out into intergalactic space to wander the Universe alone. Others will be catapulted into the supermassive black hole at the centre of their galaxy, temporarily reigniting the black hole and making the galaxy active again with a blazing core. This could well be the ultimate fate of our own Sun, as the Milky Way Galaxy and the Andromeda Galaxy collide. Every galaxy, it is believed, has a supermassive black hole and as these vortexes of space–time interact, releasing a blaze of cosmic energy, an even more massive, voracious cosmic dustbin will be forged. After perhaps 100,000 billion years, all the cosmic gas will have been either pulled into existing stars or sucked into gigantic black holes. With no gas clouds left to collapse into new stars, stellar activity will be coming to an end. The lights will be going out across the cosmos, leaving space to contain nothing but highly isolated collections of dead stars: white dwarfs, neutron stars, and black holes both large and small. These stellar cadavers will occasionally collide, releasing a sudden flash of brilliance, but, by and large, no light will be shining through the Universe. ‘Eventually all black holes will simply disappear in a puff of radiation.’ PAUL DAVIS CONTEMPORARY COSMOLOGIST But this might not be quite the end. There are hints that protons, which make up a vital constituent of each atomic nucleus, may not be permanently stable. If they eventually decay, all the atoms in the Universe will disintegrate, leaving a sea of subatomic particles. Chemical reactions and nuclear reactions will become impossible, and the only structures able to form will be black holes as the particles clump together. Even the black holes may not last forever. They may gradually evaporate, radiating their matter content back into the Universe in the form of more subatomic particles. Such a phenomenon, known as ‘Hawking radiation’ after its proponent (see What is a Black Hole?), would take place over an inconceivably long time period, perhaps a googol years (1 followed by 100 zeros). So, the long-term fate of an open Universe is to become a dilute sea of particles, all at approximately the same low temperature and unable to react with one another. Such a state is known as the ‘heat death’ of the Universe. The big rip We have talked of two scenarios, but between the open Universe and the closed Universe eventualities is the unique possibility of a ‘flat’ Universe. Its two-dimensional analogy is simply a flat sheet that extends forever in all directions. Whilst there are myriad types of both open and closed Universes, depending upon the density of matter, there is only one flat Universe. It corresponds to the Universe containing exactly the critical density of matter. Its ultimate fate would be identical to the open Universe scenario –the dilute sea of cold particles –but it is a highly improbable case, given how finely tuned the Universe would need to have been during the Big Bang to create such a precise amount of matter. Nevertheless, it is type of Universe that cosmologists believe that we live in. Astronomers have been able to analyse ripples in the microwave background to discover the geometry of the Universe, and have found it to be almost perfectly flat. But this result did not balance with their inventories of matter until they discovered the acceleration of space and decided that some form of ‘dark energy’ must also permeate the cosmos, making up the deficit (see What is Dark Energy?).

>Until we know exactly what dark energy is, we have no way of being certain how it behaves and so the straightforward picture of a flat Universe expanding forever may not be true after all. Dark energy may turn off, gain strength or even reverse its effects. Astronomers need to be able to determine which behaviour pattern dark energy will follow; until then they cannot rule out any possibility for the fate of the Universe. If the dark energy effect remains constant, the expansion of space will continue to accelerate, shortening the time it will take for the galaxies to disappear from view. But if the dark energy were to reverse its behaviour, it would increase the strength of gravity in the Universe and could pull everything back into a big crunch. The most bizarre option would be if dark energy continually increased with time. If this were to happen, the Universe’s expansion would accelerate at an ever-increasing rate. After driving all of the galaxies so far away that we could no longer see them, dark energy would then go to work on the Milky Way. It would disrupt our Galaxy and even drive planets out of their orbits around stars. Ever-strengthening dark energy would then pull apart the stars, followed closely by the planets. Finally, it would explode the very particles that make up matter. Astronomers call this nightmare scenario ‘the big rip’.

>It seems somewhat depressing that there are currently no hypotheses that preserve the Universe in its present state. The ‘steady state theory’, popular in the mid-20th century, relied on the continuous creation of matter to fill the gaps created by the expanding Universe. But the theory was disproved by the discovery of the cosmic microwave background radiation and its interpretation as the residual energy of the Big Bang. The majority of astronomers now believe that the most likely fate for the Universe is to expand forever and suffer a heat death. But none of the scenarios presented in this chapter can yet be ruled out.